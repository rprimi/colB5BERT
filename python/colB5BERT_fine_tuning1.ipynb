{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOm2nC7bpNv5TG4d9SM1XPb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "54bf724533124bc987cb091453846d25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e092197f7aa54c12892ed92b4ca96c0c",
              "IPY_MODEL_2b354dddc72a4b6589d498d4988b5cc4",
              "IPY_MODEL_046d8b92d5fa4097bed26e0ea46c362c"
            ],
            "layout": "IPY_MODEL_13bec119b5714b04a25de04a38d9dad7"
          }
        },
        "e092197f7aa54c12892ed92b4ca96c0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73c46af8668645428a5930822d899b71",
            "placeholder": "​",
            "style": "IPY_MODEL_390504fead8e46e3810ca28be09e3236",
            "value": "Epochs:   0%"
          }
        },
        "2b354dddc72a4b6589d498d4988b5cc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a8dc337148144a6831080b3133554ed",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_930430119ebd49af813f7b4b27e6ca91",
            "value": 0
          }
        },
        "046d8b92d5fa4097bed26e0ea46c362c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91abad42efe94abe8ffa2f347762d5f5",
            "placeholder": "​",
            "style": "IPY_MODEL_e8fa3bb52e4e47ff9b2636cc00354d7d",
            "value": " 0/4 [00:00&lt;?, ?it/s]"
          }
        },
        "13bec119b5714b04a25de04a38d9dad7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73c46af8668645428a5930822d899b71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "390504fead8e46e3810ca28be09e3236": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a8dc337148144a6831080b3133554ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "930430119ebd49af813f7b4b27e6ca91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "91abad42efe94abe8ffa2f347762d5f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8fa3bb52e4e47ff9b2636cc00354d7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5572432a9bad4cc0bc3674c488082bdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8fcbc51460dc43699e5007d8ec09ed38",
              "IPY_MODEL_3b92feb4a7984e63bf21da5b132470a1",
              "IPY_MODEL_d2067a7a3ce942a2ab6c3a4d65083c49"
            ],
            "layout": "IPY_MODEL_54f32bcc04614575a2c938be93e0ffc8"
          }
        },
        "8fcbc51460dc43699e5007d8ec09ed38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66797a85101442c199c5fd3033a3442c",
            "placeholder": "​",
            "style": "IPY_MODEL_f30c6627b8014cbc823bd3698eaae17a",
            "value": "Training batches (Epoch 0):  14%"
          }
        },
        "3b92feb4a7984e63bf21da5b132470a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf68313c73364768bc5aacd2a6779fac",
            "max": 31055,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6d94335875064257b6be5a00d3c07a76",
            "value": 4481
          }
        },
        "d2067a7a3ce942a2ab6c3a4d65083c49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae07f445908c4be7bb58c99ab1f2470c",
            "placeholder": "​",
            "style": "IPY_MODEL_b97c42dd71f74685a5f9703b3be09cd2",
            "value": " 4481/31055 [56:40&lt;5:36:09,  1.32it/s]"
          }
        },
        "54f32bcc04614575a2c938be93e0ffc8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66797a85101442c199c5fd3033a3442c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f30c6627b8014cbc823bd3698eaae17a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf68313c73364768bc5aacd2a6779fac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d94335875064257b6be5a00d3c07a76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ae07f445908c4be7bb58c99ab1f2470c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b97c42dd71f74685a5f9703b3be09cd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rprimi/colB5BERT/blob/main/python/colB5BERT_fine_tuning1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **colB5BERT:** Fine tuning colBERT with Big Five dataset\n",
        "\n",
        "Solução Baseada no notebook do Pedro Genco"
      ],
      "metadata": {
        "id": "ijk96nJ77H3t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZUio6VJkQfQ"
      },
      "outputs": [],
      "source": [
        "!pip3 install transformers hnswlib evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "z1wco2iUqepZ",
        "outputId": "2b77476d-90b1-4d79-8bb9-24ae2945d7b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/rprimi/colB5BERT.git\n",
        "\n",
        "%cd /content/colB5BERT\n",
        "!git pull\n"
      ],
      "metadata": {
        "id": "H8evBd6sMsqT",
        "outputId": "e57da0d5-794a-4ecb-f54a-041bfae89a51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'colB5BERT'...\n",
            "remote: Enumerating objects: 290, done.\u001b[K\n",
            "remote: Counting objects: 100% (94/94), done.\u001b[K\n",
            "remote: Compressing objects: 100% (76/76), done.\u001b[K\n",
            "remote: Total 290 (delta 70), reused 18 (delta 18), pack-reused 196\u001b[K\n",
            "Receiving objects: 100% (290/290), 32.72 MiB | 13.14 MiB/s, done.\n",
            "Resolving deltas: 100% (185/185), done.\n",
            "/content/colB5BERT\n",
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Libraries"
      ],
      "metadata": {
        "id": "3leM67vs73wD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wIOgkGQmkk-5"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "import json\n",
        "\n",
        "import hnswlib\n",
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from collections import defaultdict\n",
        "from evaluate import load\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
        "\n",
        "from torch.utils import data\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BatchEncoding\n",
        "import torch\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import get_linear_schedule_with_warmup, get_cosine_with_hard_restarts_schedule_with_warmup, AdamW\n",
        "\n",
        "import textwrap\n",
        "import pickle\n",
        "import h5py\n",
        "import logging\n",
        "\n",
        "from transformers import BertModel, BertTokenizer\n",
        "\n",
        "import torch\n",
        "from torch.nn.functional import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# sys.path.append('/content/colB5BERT/python/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "JAqaTuEMVTsp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8044063c-da33-4db8-88ce-52450d0297af"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Jun 27 23:13:42 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0    44W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.read_csv('/content/drive/MyDrive/colB5BERT/dataset_test_positive.tsv', sep='\\t')\n",
        "df_train = pd.read_csv('/content/drive/MyDrive/colB5BERT/dataset_train_positive.tsv', sep='\\t')\n",
        "\n",
        "\n",
        "print(f'Train dataset: {len(df_train)}')\n",
        "print(f'Test dataset: {len(df_test)}')\n",
        "# Sample data to test the pipeline\n",
        "\n",
        "#df_test = df_test.sample(frac=0.08)\n",
        "#df_train = df_train.sample(frac=0.08)\n",
        "\n",
        "df_test = df_test.sample(n = 1000)\n",
        "df_train = df_train.sample(n = 4000)"
      ],
      "metadata": {
        "id": "fT5wqWymwppW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c6c72e8-fc31-4ca1-a675-0d79265757f9"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset: 993759\n",
            "Test dataset: 158594\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, base_model, use_cls = False):\n",
        "    super().__init__()\n",
        "    self.base_model = base_model\n",
        "    self.use_cls = use_cls\n",
        "\n",
        "  def forward(self, x):\n",
        "    embeddings = self.base_model(**x)\n",
        "\n",
        "    if self.use_cls:\n",
        "      embeddings = embeddings[0][:, 0, :]\n",
        "    else:\n",
        "      embeddings = embeddings[0] #First element of model_output contains all token embeddings\n",
        "      attention_mask = x[\"attention_mask\"]\n",
        "      input_mask_expanded = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
        "      embeddings = torch.sum(embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "GHJyPXGMivnW"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This uses a lot of RAM. I will use the other solution."
      ],
      "metadata": {
        "id": "qnnj7buEP3tN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Parametros de max_length baseados no notebook do Marcos Piau\n",
        "class QueriesDataset(Dataset):\n",
        "  def __init__(self, queries, tokenizer):\n",
        "    self.queries = queries\n",
        "    self.tokenizer = tokenizer\n",
        "    self.cache = {}\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.queries)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if self.cache.get(idx) is None:\n",
        "      self.cache[idx] = self.tokenizer(\n",
        "                            self.queries[idx],\n",
        "                            padding=\"max_length\",\n",
        "                            truncation=True,\n",
        "                            max_length=32,\n",
        "                            return_tensors='pt'\n",
        "                        )\n",
        "\n",
        "    output = {\n",
        "        \"input_ids\": self.cache[idx][\"input_ids\"].squeeze(),\n",
        "        \"attention_mask\": self.cache[idx][\"attention_mask\"].squeeze()\n",
        "    }\n",
        "    return output\n",
        "\n",
        "class DocumentsDataset(Dataset):\n",
        "  def __init__(self, relevant_documents,  tokenizer):\n",
        "    self.relevant_documents = relevant_documents\n",
        "   # self.not_relevant_documents = not_relevant_documents\n",
        "    self.tokenizer = tokenizer\n",
        "    self.relevant_cache = {}\n",
        "  #  self.not_relevant_cache = {}\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.relevant_documents)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if self.relevant_cache.get(idx) is None:\n",
        "      self.relevant_cache[idx] = self.tokenizer(\n",
        "                                    self.relevant_documents[idx],\n",
        "                                    padding=\"max_length\",\n",
        "                                    truncation=True,\n",
        "                                    max_length=512,\n",
        "                                    return_tensors='pt'\n",
        "                                )\n",
        "\n",
        "#    if self.not_relevant_cache.get(idx) is None:\n",
        "#      self.not_relevant_cache[idx] = self.tokenizer(\n",
        "#                                      self.not_relevant_documents[idx],\n",
        "#                                      padding=\"max_length\",\n",
        "#                                      truncation=True,\n",
        "#                                      max_length=256,\n",
        "#                                      return_tensors='pt'\n",
        "#                                  )\n",
        "\n",
        "    sample = {\n",
        "              \"relevant_input_ids\": self.relevant_cache[idx][\"input_ids\"].squeeze(),\n",
        "              \"relevant_attention_mask\": self.relevant_cache[idx][\"attention_mask\"].squeeze()\n",
        "#              \"not_relevant_input_ids\": self.not_relevant_cache[idx][\"input_ids\"].squeeze(),\n",
        "#              \"not_relevant_attention_mask\": self.not_relevant_cache[idx][\"attention_mask\"].squeeze(),\n",
        "             }\n",
        "    return sample"
      ],
      "metadata": {
        "id": "YaY-i8TBj4FQ"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove In-Memory Caching: Currently, the QueriesDataset and DocumentsDataset classes are using caching (self.cache and self.relevant_cache). These caches are storing tokenized sequences in memory, which can consume a significant amount of RAM especially when dealing with large datasets. You might consider tokenizing on-the-fly in the __getitem__ method, and not storing the tokenized results. The downside is that it will increase the I/O and CPU usage because tokenization will happen every time a sample is accessed.\n",
        "Modifying the QueriesDataset and DocumentsDataset classes to perform on-the-fly tokenization without caching, the modified classes would look like:"
      ],
      "metadata": {
        "id": "4jkxtAY4OVqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QueriesDataset(Dataset):\n",
        "  def __init__(self, queries, tokenizer):\n",
        "    self.queries = queries\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.queries)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    tokenized_query = self.tokenizer(\n",
        "        self.queries[idx],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=32,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    output = {\n",
        "        \"input_ids\": tokenized_query[\"input_ids\"].squeeze(),\n",
        "        \"attention_mask\": tokenized_query[\"attention_mask\"].squeeze()\n",
        "    }\n",
        "    return output\n",
        "\n",
        "class DocumentsDataset(Dataset):\n",
        "  def __init__(self, relevant_documents,  tokenizer):\n",
        "    self.relevant_documents = relevant_documents\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.relevant_documents)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    tokenized_doc = self.tokenizer(\n",
        "        self.relevant_documents[idx],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    sample = {\n",
        "        \"relevant_input_ids\": tokenized_doc[\"input_ids\"].squeeze(),\n",
        "        \"relevant_attention_mask\": tokenized_doc[\"attention_mask\"].squeeze()\n",
        "    }\n",
        "    return sample\n"
      ],
      "metadata": {
        "id": "h1WbwYiWOVJS"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eval and Training loop"
      ],
      "metadata": {
        "id": "AQMr9xzMCUAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval(eval_dataloader_query, eval_dataloader_doc, model_query, model_doc, device):\n",
        "  loss_val = 0\n",
        "  model_query.eval()\n",
        "  model_doc.eval()\n",
        "  for query_batch, doc_batch in tqdm(zip(eval_dataloader_query, eval_dataloader_doc), total=len(eval_dataloader_query), desc='Validation batches'):\n",
        "    with torch.no_grad():\n",
        "      query_input = {\n",
        "          \"input_ids\": query_batch[\"input_ids\"].to(device),\n",
        "          \"attention_mask\": query_batch[\"attention_mask\"].to(device)\n",
        "      }\n",
        "      query_emb = model_query(query_input)\n",
        "\n",
        "      relevant_input = {\n",
        "          \"input_ids\": doc_batch[\"relevant_input_ids\"].to(device),\n",
        "          \"attention_mask\": doc_batch[\"relevant_attention_mask\"].to(device)\n",
        "      }\n",
        "\n",
        "#      not_relevant_input = {\n",
        "#          \"input_ids\": doc_batch[\"not_relevant_input_ids\"].to(device),\n",
        "#          \"attention_mask\": doc_batch[\"not_relevant_attention_mask\"].to(device)\n",
        "#      }\n",
        "\n",
        "      relevant_embeddings = model_doc(relevant_input)\n",
        "#      not_relevant_embeddings = model_doc(not_relevant_input)\n",
        "\n",
        "      ## Split in two parts: dot product of query and relevant, and dot product of query and not rel\n",
        "\n",
        "      sim = torch.matmul(query_emb, relevant_embeddings.T) # B, H x H, B = B, B\n",
        "      exp_sim = torch.exp(sim) # B, B\n",
        "      relevant_sim = torch.diagonal(exp_sim, 0) # B,\n",
        "      sum_all_sims = torch.sum(exp_sim, dim=1) # B,\n",
        "\n",
        "      # not_relevant_sim = torch.sum(query_emb * not_relevant_embeddings, dim=1) # B,\n",
        "      # exp_not_relevant_sim = torch.exp(not_relevant_sim) # B,\n",
        "      # sum_all_sims = sum_all_sims + exp_not_relevant_sim # B,\n",
        "\n",
        "      log_loss = -1 * torch.log(relevant_sim / sum_all_sims)\n",
        "      loss = torch.mean(log_loss)\n",
        "      loss_val += loss.cpu().item()\n",
        "\n",
        "  print(f\"Eval loss: {loss_val / len(eval_dataloader_query)}\")\n",
        "\n",
        "\n",
        "def train(train_dataloader_query, train_dataloader_doc,\n",
        "          eval_dataloader_query, eval_dataloader_doc,\n",
        "          model_query, model_doc, optimizer, scheduler, epochs, device, accumulation_steps=4):\n",
        "  for epoch in tqdm(range(epochs), desc='Epochs'):\n",
        "    model_query.train()\n",
        "    model_doc.train()\n",
        "    train_loss = 0\n",
        "    for step, (query_batch, doc_batch) in enumerate(tqdm(zip(train_dataloader_query, train_dataloader_doc), total=len(train_dataloader_query), desc=f'Training batches (Epoch {epoch})')):\n",
        "    # for query_batch, doc_batch in tqdm(zip(train_dataloader_query, train_dataloader_doc), total=len(train_dataloader_query), desc=f'Training batches (Epoch {epoch})'):\n",
        "      optimizer.zero_grad() if step % accumulation_steps == 0 else None  # Zero gradients every 'accumulation_steps' steps\n",
        "      # ... the rest of your code here ...\n",
        "      query_input = {\n",
        "          \"input_ids\": query_batch[\"input_ids\"].to(device),\n",
        "          \"attention_mask\": query_batch[\"attention_mask\"].to(device)\n",
        "      }\n",
        "      query_emb = model_query(query_input)\n",
        "\n",
        "      relevant_input = {\n",
        "          \"input_ids\": doc_batch[\"relevant_input_ids\"].to(device),\n",
        "          \"attention_mask\": doc_batch[\"relevant_attention_mask\"].to(device)\n",
        "      }\n",
        "\n",
        "   #   not_relevant_input = {\n",
        "   #       \"input_ids\": doc_batch[\"not_relevant_input_ids\"].to(device),\n",
        "   #       \"attention_mask\": doc_batch[\"not_relevant_attention_mask\"].to(device)\n",
        "   #   }\n",
        "\n",
        "      relevant_embeddings = model_doc(relevant_input)\n",
        "   #   not_relevant_embeddings = model_doc(not_relevant_input)\n",
        "\n",
        "      ## Split in two parts: dot product of query and relevant, and dot product of query and not rel\n",
        "\n",
        "      sim = torch.matmul(query_emb, relevant_embeddings.T) # B, H x H, B = B, B\n",
        "      exp_sim = torch.exp(sim) # B, B\n",
        "      relevant_sim = torch.diagonal(exp_sim, 0) # B,\n",
        "      sum_all_sims = torch.sum(exp_sim, dim=1) # B,\n",
        "\n",
        "      # not_relevant_sim = torch.sum(query_emb * not_relevant_embeddings, dim=1) # B,\n",
        "      # exp_not_relevant_sim = torch.exp(not_relevant_sim) # B,\n",
        "      # sum_all_sims = sum_all_sims + exp_not_relevant_sim # B,\n",
        "\n",
        "      log_loss = -1 * torch.log(relevant_sim / sum_all_sims)\n",
        "      loss = torch.mean(log_loss)\n",
        "      loss.backward()\n",
        "      if (step + 1) % accumulation_steps == 0 or (step + 1) == len(train_dataloader_query):  # Perform the optimization step every 'accumulation_steps' steps or at the last step\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "      train_loss += loss.cpu().item()\n",
        "\n",
        "    print(f\"Epoch {epoch}\")\n",
        "    print(f\"Train loss: {train_loss / len(train_dataloader_query)}\")\n",
        "\n",
        "    # Save models at the end of each epoch\n",
        "    torch.save(model_query.state_dict(), f\"/content/drive/MyDrive/colB5BERT/model_query_epoch_{epoch}.pt\")\n",
        "    torch.save(model_doc.state_dict(), f\"/content/drive/MyDrive/colB5BERT/model_doc_epoch_{epoch}.pt\")\n",
        "\n",
        "    eval(eval_dataloader_query, eval_dataloader_doc, model_query, model_doc, device)\n",
        "  return model_query, model_doc"
      ],
      "metadata": {
        "id": "IfwB_MwNCYQi"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "    \"use_cls\": False,\n",
        "    \"similarity_function\": \"dot\",\n",
        "    \"model_name\": 'neuralmind/bert-base-portuguese-cased',\n",
        "}"
      ],
      "metadata": {
        "id": "YI_PbQnnhiX2"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(params[\"model_name\"])\n",
        "model_query = AutoModel.from_pretrained(params[\"model_name\"])\n",
        "encoder_query = Encoder(model_query, False)\n",
        "\n",
        "model_doc = AutoModel.from_pretrained(params[\"model_name\"])\n",
        "encoder_doc = Encoder(model_doc, False)"
      ],
      "metadata": {
        "id": "IykAQibKgn_C",
        "outputId": "32523d16-b975-46bd-93e5-1f4b24228b89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_dataset_train = QueriesDataset(queries = list(df_train['item_pt_text']), tokenizer = tokenizer)\n",
        "doc_datasets_train = DocumentsDataset(list(df_train['texto_dividido']),  tokenizer)\n",
        "\n",
        "query_dataset_eval = QueriesDataset(queries = list(df_test['item_pt_text']), tokenizer = tokenizer)\n",
        "doc_datasets_eval = DocumentsDataset(list(df_test['texto_dividido']), tokenizer)"
      ],
      "metadata": {
        "id": "QhKrjgFhN55p"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_query_dataloader = DataLoader(query_dataset_train, batch_size = 32, shuffle=False)\n",
        "train_doc_dataloader = DataLoader(doc_datasets_train, batch_size = 32, shuffle=False)\n",
        "\n",
        "eval_query_dataloader = DataLoader(query_dataset_eval, batch_size = 32, shuffle=False)\n",
        "eval_doc_dataloader = DataLoader(doc_datasets_eval, batch_size = 32, shuffle=False)"
      ],
      "metadata": {
        "id": "ihe0Ag5bGul2"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJvv18osmhVe",
        "outputId": "80691ac3-189f-4d6e-ea2f-d870389999ce"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "158"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "encoder_doc = encoder_doc.to(device)\n",
        "encoder_query = encoder_query.to(device)\n",
        "optim_params = list(encoder_query.parameters()) + list(encoder_doc.parameters())\n",
        "optimizer = torch.optim.AdamW(optim_params, lr=5e-5)\n",
        "\n",
        "epochs = 4\n",
        "num_training_steps = epochs * len(train_query_dataloader)\n",
        "num_warmup_steps = int(num_training_steps * 0.1)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n",
        "\n",
        "encoder_query, encoder_doc = train(train_query_dataloader, train_doc_dataloader,\n",
        "              eval_query_dataloader, eval_doc_dataloader,\n",
        "              encoder_query, encoder_doc, optimizer, scheduler, epochs, device, accumulation_steps=4)\n",
        "\n",
        "# encoder_query, encoder_doc = train(overfit_query_dataloader, overfit_doc_dataloader,\n",
        "#                                    overfit_query_dataloader, overfit_doc_dataloader,\n",
        "#                                    encoder_query, encoder_doc, optimizer, 100, device)\n",
        "\n"
      ],
      "metadata": {
        "id": "_TxwAyJfAgmP",
        "outputId": "52b67ed8-7134-49e2-c0c5-bd91b7f76cdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109,
          "referenced_widgets": [
            "54bf724533124bc987cb091453846d25",
            "e092197f7aa54c12892ed92b4ca96c0c",
            "2b354dddc72a4b6589d498d4988b5cc4",
            "046d8b92d5fa4097bed26e0ea46c362c",
            "13bec119b5714b04a25de04a38d9dad7",
            "73c46af8668645428a5930822d899b71",
            "390504fead8e46e3810ca28be09e3236",
            "2a8dc337148144a6831080b3133554ed",
            "930430119ebd49af813f7b4b27e6ca91",
            "91abad42efe94abe8ffa2f347762d5f5",
            "e8fa3bb52e4e47ff9b2636cc00354d7d",
            "5572432a9bad4cc0bc3674c488082bdd",
            "8fcbc51460dc43699e5007d8ec09ed38",
            "3b92feb4a7984e63bf21da5b132470a1",
            "d2067a7a3ce942a2ab6c3a4d65083c49",
            "54f32bcc04614575a2c938be93e0ffc8",
            "66797a85101442c199c5fd3033a3442c",
            "f30c6627b8014cbc823bd3698eaae17a",
            "bf68313c73364768bc5aacd2a6779fac",
            "6d94335875064257b6be5a00d3c07a76",
            "ae07f445908c4be7bb58c99ab1f2470c",
            "b97c42dd71f74685a5f9703b3be09cd2"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epochs:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "54bf724533124bc987cb091453846d25"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training batches (Epoch 0):   0%|          | 0/31055 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5572432a9bad4cc0bc3674c488082bdd"
            }
          },
          "metadata": {}
        }
      ]
    }
  ]
}