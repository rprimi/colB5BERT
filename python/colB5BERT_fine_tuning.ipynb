{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNfma7gov/uDmqeoD0jJ0PJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rprimi/colB5BERT/blob/main/python/colB5BERT_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **colB5BERT:** Fine tuning colBERT with Big Five dataset"
      ],
      "metadata": {
        "id": "ijk96nJ77H3t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Notebook inspirado no buscador denso de Leandro Carísio Fernandes"
      ],
      "metadata": {
        "id": "Si4U-9UevVK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "z1wco2iUqepZ",
        "outputId": "6ca23e92-3a6f-4500-9724-7f168023fb6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZUio6VJkQfQ"
      },
      "outputs": [],
      "source": [
        "!pip3 install transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/rprimi/colB5BERT.git\n",
        "\n",
        "%cd /content/colB5BERT\n",
        "!git pull\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8evBd6sMsqT",
        "outputId": "c778496a-a5f8-4e58-c3e8-538e2415b032"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'colB5BERT'...\n",
            "remote: Enumerating objects: 242, done.\u001b[K\n",
            "remote: Counting objects: 100% (46/46), done.\u001b[K\n",
            "remote: Compressing objects: 100% (28/28), done.\u001b[K\n",
            "remote: Total 242 (delta 34), reused 18 (delta 18), pack-reused 196\u001b[K\n",
            "Receiving objects: 100% (242/242), 32.67 MiB | 4.33 MiB/s, done.\n",
            "Resolving deltas: 100% (149/149), done.\n",
            "/content/colB5BERT\n",
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Libraries"
      ],
      "metadata": {
        "id": "3leM67vs73wD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "wIOgkGQmkk-5"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/colB5BERT/python/')\n",
        "\n",
        "import utils\n",
        "import vsm\n",
        "import sst\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import textwrap\n",
        "import pickle\n",
        "import h5py\n",
        "import logging\n",
        "\n",
        "from transformers import BertModel, BertTokenizer\n",
        "from transformers import RobertaModel, RobertaTokenizer\n",
        "\n",
        "import torch\n",
        "from torch.nn.functional import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils import data\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BatchEncoding\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModel\n",
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import get_linear_schedule_with_warmup, get_cosine_with_hard_restarts_schedule_with_warmup, AdamW\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "JAqaTuEMVTsp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30f2fc1e-7008-4ced-bd2f-dd9bf9a2b8de"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jun 26 19:54:00 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P0    49W / 400W |   1975MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.read_csv('/content/drive/MyDrive/colB5BERT/dataset_test.tsv', sep='\\t')\n",
        "df_train = pd.read_csv('/content/drive/MyDrive/colB5BERT/dataset_train.tsv', sep='\\t')\n",
        "\n",
        "\n",
        "# Separa os conjuntos de treinamento e validação\n",
        "queries_train = df_train['item_pt_text'].tolist()\n",
        "docs_train = df_train['texto_dividido'].tolist()\n",
        "positive_ex_train = df_train['postive_ex'].tolist()\n",
        "\n",
        "\n",
        "queries_val = df_test['item_pt_text'].tolist()\n",
        "docs_val = df_test['texto_dividido'].tolist()\n",
        "positive_ex_val = df_test['postive_ex'].tolist()\n",
        "\n",
        "type(queries_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fT5wqWymwppW",
        "outputId": "55eadb60-0d7f-4481-8d7b-9396e2668bda"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aq9LmOj4Fi4x"
      },
      "source": [
        "## Fine-tuning dos encoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7oebY_0_y-8"
      },
      "source": [
        "Define os datasets e dataloaders:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "b94xkxC-F5k2"
      },
      "outputs": [],
      "source": [
        "# Definição do Dataset\n",
        "class Dataset(data.Dataset):\n",
        "    # Recebe dois vetores de textos e um vetor de labels\n",
        "    def __init__(self, tokenizer, textos, labels, max_seq_length = max_length):\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.tokenizer = tokenizer\n",
        "        self.textos = textos\n",
        "        self.labels = labels\n",
        "        self.cache = {}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.textos)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        self.cache[str(idx)] = self.cache.get(str(idx),\n",
        "                   (self.tokenizer(self.textos[idx],\n",
        "                                  padding=True,\n",
        "                                  truncation=True,\n",
        "                                  max_length=self.max_seq_length\n",
        "                                  ),\n",
        "                    self.labels[idx])\n",
        "                   )\n",
        "        return self.cache[str(idx)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "fC1YppfOGXur"
      },
      "outputs": [],
      "source": [
        "# Create datasets\n",
        "bert_weights_name = 'neuralmind/bert-base-portuguese-cased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(bert_weights_name)\n",
        "\n",
        "dataset_queries_train = Dataset(tokenizer, queries_train, positive_ex_train)\n",
        "dataset_docs_train = Dataset(tokenizer, docs_train, positive_ex_train)\n",
        "\n",
        "dataset_queries_val = Dataset(tokenizer, queries_val, positive_ex_val)\n",
        "dataset_docs_val = Dataset(tokenizer, docs_val, positive_ex_val)\n",
        "\n",
        "\n",
        "# Dataloaders para os datasets\n",
        "\n",
        "#collate_fn = lambda batch: BatchEncoding(tokenizer.pad(batch, return_tensors='pt'))\n",
        "def collate_fn(batch):\n",
        "    #print('Dentro de collate_fn')\n",
        "    #print(BatchEncoding(tokenizer.pad(batch, return_tensors='pt')))\n",
        "    return BatchEncoding(tokenizer.pad(batch, return_tensors='pt'))\n",
        "\n",
        "# collate function that also handles the labels\n",
        "def collate_fn(batch):\n",
        "    inputs = [item[0] for item in batch]\n",
        "    labels = [item[1] for item in batch]\n",
        "    return BatchEncoding(tokenizer.pad(inputs, return_tensors='pt')), torch.tensor(labels)\n",
        "\n",
        "\n",
        "# Create dataloaders\n",
        "dataloader_queries_train = DataLoader(dataset_queries_train, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "dataloader_docs_train = DataLoader(dataset_docs_train, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "dataloader_queries_val = DataLoader(dataset_queries_val, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "dataloader_docs_val = DataLoader(dataset_docs_val, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "\n",
        "\n",
        "# Adapt your DataLoader object\n",
        "#class PositiveExDataLoader:\n",
        "#    def __init__(self, dataloader, positive_ex):\n",
        "#        self.dataloader = dataloader\n",
        "#        self.positive_ex = positive_ex\n",
        "#\n",
        "#    def __iter__(self):\n",
        "#        for batch, pos_ex in zip(self.dataloader, self.positive_ex):\n",
        "#            yield batch, pos_ex\n",
        "#    def __len__(self):\n",
        "#        return len(self.dataloader)\n",
        "## Wrap your original dataloaders\n",
        "#dataloader_queries_train = PositiveExDataLoader(dataloader_queries_train, positive_ex_train)\n",
        "#dataloader_docs_train = PositiveExDataLoader(dataloader_docs_train, positive_ex_train)\n",
        "#\n",
        "#dataloader_queries_val = PositiveExDataLoader(dataloader_queries_val, positive_ex_val)\n",
        "#dataloader_docs_val = PositiveExDataLoader(dataloader_docs_val, positive_ex_val)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGouSrQgF-sn"
      },
      "source": [
        "Carrega os modelos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "FpWhDxhFGh2G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3752a81a-274b-4d95-d342-33a99d202da0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Se tiver que treinar os modelos, abre\n",
        "model_query = AutoModel.from_pretrained(bert_weights_name).to(device)\n",
        "model_doc = AutoModel.from_pretrained(bert_weights_name).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xinVUAMdGnoa"
      },
      "source": [
        "Define função pro cálculo da loss (modifiquei essa função para trabalhar com um vetor indicando quais pares são relevantes e quais não são:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "nE_Kk3ZeGp2L"
      },
      "outputs": [],
      "source": [
        "# Essa função já considera o resultado via batchs:\n",
        "def compute_loss_com_gradiente(model_query, tokenized_queries, model_doc, tokenized_docs, positive_ex):\n",
        "    outputs_queries = model_query(**tokenized_queries[0].to(device))\n",
        "    outputs_docs    = model_doc(**tokenized_docs[0].to(device))\n",
        "\n",
        "    # Extrai a última camada oculta associada ao token [CLS]\n",
        "    tcls_queries = outputs_queries.last_hidden_state[:, 0, :]\n",
        "    tcls_docs    = outputs_docs.last_hidden_state[:, 0, :]\n",
        "\n",
        "    # Normaliza os tensores\n",
        "    #tcls_queries = tcls_queries / torch.norm(tcls_queries, dim=1, keepdim=True)\n",
        "    #tcls_docs = tcls_docs / torch.norm(tcls_docs, dim=1, keepdim=True)\n",
        "\n",
        "    # Agora é necessário calcular a loss. Para isso, o primeiro passo é\n",
        "    # calcular a similaridade entre uma query e documento (sim(q, d))\n",
        "    similaridade = torch.matmul(tcls_queries, torch.transpose(tcls_docs, 0, 1))\n",
        "\n",
        "    # Calcula a exponencial da similaridade\n",
        "    exp_sim = torch.exp(similaridade)\n",
        "\n",
        "    # Calcula a loss\n",
        "    # We are now considering only the positive examples (where positive_ex is 1)\n",
        "    positive_exp_sim = exp_sim * positive_ex.to(device)\n",
        "    soma_linhas = positive_exp_sim.sum(dim=1)\n",
        "    diagonal = torch.diag(positive_exp_sim)\n",
        "    log_loss = -1* torch.log(diagonal/soma_linhas)\n",
        "\n",
        "    loss = torch.mean(log_loss)\n",
        "    return loss\n",
        "\n",
        "def compute_loss_sem_gradiente(model_query, tokenized_queries, model_doc, tokenized_docs, positive_ex):\n",
        "    with torch.no_grad():\n",
        "        return compute_loss_com_gradiente(model_query, tokenized_queries, model_doc, tokenized_docs, positive_ex)\n",
        "\n",
        "def compute_loss_dataloaders(model_query, dataloader_query, model_doc, dataloader_docs, positive_ex):\n",
        "    loss = 0\n",
        "    n_batches = 0\n",
        "    for batch_query, batch_docs in zip(dataloader_query, dataloader_docs):\n",
        "        loss = loss + compute_loss_sem_gradiente(model_query, batch_query, model_doc, batch_docs, positive_ex)\n",
        "        n_batches += 1\n",
        "    return loss/n_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xTW39DGTuXY"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Só pra medir o tempo que ele demora para calcular a loss em todo o dataset de treinamento\n",
        "model_query.eval()\n",
        "model_doc.eval()\n",
        "print(f'Loss de treinamento: {compute_loss_dataloaders(model_query, dataloader_queries_train, model_doc, dataloader_docs_train, dataloader_queries_train.dataset.labels)}')\n",
        "print(f'Loss de validação: {compute_loss_dataloaders(model_query, dataloader_queries_val, model_doc, dataloader_docs_val, dataloader_queries_val.dataset.labels)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KST5dRqWGxff"
      },
      "source": [
        "Agora treina os dois encoders simulatenamente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "JrzMILg4GxDz"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "max_length = 512\n",
        "batch_size = 50\n",
        "epochs = 20\n",
        "lr = 2e-5\n",
        "\n",
        "treinar_e_salvar_modelos  = True\n",
        "# Diretório onde vai salvando o modelo a cada época\n",
        "dir_modelos = '/content/drive/MyDrive/colB5BERT'\n",
        "\n",
        "\n",
        "if treinar_e_salvar_modelos:\n",
        "  # Training loop\n",
        "  optimizer_query = AdamW(model_query.parameters(), lr=lr)\n",
        "  optimizer_doc = AdamW(model_doc.parameters(), lr=lr)\n",
        "\n",
        "  num_training_steps = epochs * len(dataloader_queries_train)\n",
        "  num_warmup_steps = int(num_training_steps * 0.1)\n",
        "\n",
        "  # get_linear_schedule_with_warmup get_cosine_with_hard_restarts_schedule_with_warmup\n",
        "  scheduler_query = get_cosine_with_hard_restarts_schedule_with_warmup(optimizer_query, num_warmup_steps, num_training_steps)\n",
        "  scheduler_doc = get_cosine_with_hard_restarts_schedule_with_warmup(optimizer_doc, num_warmup_steps, num_training_steps)\n",
        "\n",
        "  for epoch in tqdm(range(epochs), desc='Epochs'):\n",
        "      model_query.train()\n",
        "      model_doc.train()\n",
        "\n",
        "      train_losses = []\n",
        "      for (batch_query, positive_ex_query), (batch_docs, positive_ex_docs) in tqdm(list(zip(dataloader_queries_train, dataloader_docs_train)), mininterval=0.5, desc='Train', disable=False):\n",
        "        optimizer_query.zero_grad()\n",
        "        optimizer_doc.zero_grad()\n",
        "\n",
        "        # Ensure positive_ex_query and positive_ex_docs are tensors and on the same device as your models\n",
        "        batch_query = batch_query.to(device)\n",
        "        batch_docs = batch_docs.to(device)\n",
        "        positive_ex_query = torch.tensor(positive_ex_query).to(device)\n",
        "        positive_ex_docs = torch.tensor(positive_ex_docs).to(device)\n",
        "\n",
        "        loss = compute_loss_com_gradiente(model_query, batch_query, model_doc, batch_docs, positive_ex_query, positive_ex_docs)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer_query.step()\n",
        "        optimizer_doc.step()\n",
        "\n",
        "        scheduler_query.step()\n",
        "        scheduler_doc.step()\n",
        "\n",
        "\n",
        "      model_query.save_pretrained(f'{dir_modelos}{epoch}/query/')\n",
        "      model_doc.save_pretrained(f'{dir_modelos}{epoch}/doc/')\n",
        "\n",
        "      model_query.eval()\n",
        "      model_doc.eval()\n",
        "\n",
        "      print(f'Loss de treinamento {epoch}: {compute_loss_dataloaders(model_query, dataloader_queries_train, model_doc, dataloader_docs_train)}')\n",
        "      print(f'Loss de validação {epoch}: {compute_loss_dataloaders(model_query, dataloader_queries_val, model_doc, dataloader_docs_val)}')"
      ]
    }
  ]
}