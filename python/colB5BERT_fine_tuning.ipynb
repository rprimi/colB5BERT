{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMr+xv+Q1OvAP87ZzwKSZ/N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c5fea8f8e554482e80851976e1244ff2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2ab7318319c84d5095b3da5a0cbc2da1",
              "IPY_MODEL_010c8436910a4313b1e6ca604a1a2003",
              "IPY_MODEL_4f12d6ca46e54c5097c6d43a1a2b88da"
            ],
            "layout": "IPY_MODEL_cccab72ecfa44b86b018e0701c6a8f67"
          }
        },
        "2ab7318319c84d5095b3da5a0cbc2da1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae1c80902daa4e24a7c8d299f3127aaf",
            "placeholder": "​",
            "style": "IPY_MODEL_99b8bd8d26aa4e52b3087830f211078e",
            "value": "Epochs:   0%"
          }
        },
        "010c8436910a4313b1e6ca604a1a2003": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9cc0374005ca45ea91ca03728b5f145d",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fad601cd79c848b7b9ce577c89e3b896",
            "value": 0
          }
        },
        "4f12d6ca46e54c5097c6d43a1a2b88da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce2323e21be34a0fbaf1fe30f44f5cb9",
            "placeholder": "​",
            "style": "IPY_MODEL_501772ffcefa4e29b3ed32be38ccfc35",
            "value": " 0/20 [00:31&lt;?, ?it/s]"
          }
        },
        "cccab72ecfa44b86b018e0701c6a8f67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae1c80902daa4e24a7c8d299f3127aaf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99b8bd8d26aa4e52b3087830f211078e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9cc0374005ca45ea91ca03728b5f145d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fad601cd79c848b7b9ce577c89e3b896": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ce2323e21be34a0fbaf1fe30f44f5cb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "501772ffcefa4e29b3ed32be38ccfc35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rprimi/colB5BERT/blob/main/python/colB5BERT_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **colB5BERT:** Fine tuning colBERT with Big Five dataset"
      ],
      "metadata": {
        "id": "ijk96nJ77H3t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Notebook inspirado no buscador denso de Leandro Carísio Fernandes"
      ],
      "metadata": {
        "id": "Si4U-9UevVK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "z1wco2iUqepZ",
        "outputId": "6f1018be-cd29-4810-83ae-539a150dd628",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mZUio6VJkQfQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc9a3896-89be-477d-db14-c07a3ff4b6d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: rpy2 in /usr/local/lib/python3.10/dist-packages (3.5.5)\n",
            "Requirement already satisfied: cffi>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from rpy2) (1.15.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from rpy2) (3.1.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from rpy2) (2022.7.1)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from rpy2) (5.0.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.10.0->rpy2) (2.21)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->rpy2) (2.1.3)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install transformers\n",
        "!pip install rpy2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/rprimi/colB5BERT.git\n",
        "\n",
        "%cd /content/colB5BERT\n",
        "!git pull\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8evBd6sMsqT",
        "outputId": "c778496a-a5f8-4e58-c3e8-538e2415b032"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'colB5BERT'...\n",
            "remote: Enumerating objects: 242, done.\u001b[K\n",
            "remote: Counting objects: 100% (46/46), done.\u001b[K\n",
            "remote: Compressing objects: 100% (28/28), done.\u001b[K\n",
            "remote: Total 242 (delta 34), reused 18 (delta 18), pack-reused 196\u001b[K\n",
            "Receiving objects: 100% (242/242), 32.67 MiB | 4.33 MiB/s, done.\n",
            "Resolving deltas: 100% (149/149), done.\n",
            "/content/colB5BERT\n",
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Libraries"
      ],
      "metadata": {
        "id": "3leM67vs73wD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "wIOgkGQmkk-5"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/colB5BERT/python/')\n",
        "\n",
        "import utils\n",
        "import vsm\n",
        "import sst\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import textwrap\n",
        "import pickle\n",
        "import h5py\n",
        "import logging\n",
        "\n",
        "from transformers import BertModel, BertTokenizer\n",
        "from transformers import RobertaModel, RobertaTokenizer\n",
        "\n",
        "import torch\n",
        "from torch.nn.functional import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "JAqaTuEMVTsp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4987c155-8e6c-439b-d184-15ad11fe6aa2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jun 26 17:30:08 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P0    44W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.read_csv('/content/drive/MyDrive/colB5BERT/dataset_test.tsv', sep='\\t')\n",
        "df_train = pd.read_csv('/content/drive/MyDrive/colB5BERT/dataset_train.tsv', sep='\\t')\n",
        "\n",
        "\n",
        "# Separa os conjuntos de treinamento e validação\n",
        "queries_train = df_train['item_pt_text'].tolist()\n",
        "docs_train = df_train['texto_dividido'].tolist()\n",
        "positive_ex_train = df_train['postive_ex'].tolist()\n",
        "\n",
        "\n",
        "queries_val = df_test['item_pt_text'].tolist()\n",
        "docs_val = df_test['texto_dividido'].tolist()\n",
        "positive_ex_val = df_test['postive_ex'].tolist()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fT5wqWymwppW",
        "outputId": "b227a572-963e-470c-8018-22222e7a91e2"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz4tGFdv-VEJ"
      },
      "source": [
        "## Preparação do ambiente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckAfaNIp-Xks"
      },
      "source": [
        "### Variáveis para controlar o fluxo do caderno\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMQ9Lro1GMKi"
      },
      "outputs": [],
      "source": [
        "treinar_e_salvar_modelos = False\n",
        "\n",
        "gerar_e_salvar_matriz_docs_trec_covid = False\n",
        "\n",
        "# Local onde fica o arquivo que contém a matriz de todos os documentos do trec_covid. Se gerar_e_salvar_matriz_docs_trec_covid = True, esse arquivo será sobrescrito com a nova geração\n",
        "arquivo_matriz_docs_trec_covid = '/content/drive/My Drive/IA368-DD_deep_learning_busca/Aula6-buscador-denso/matriz_docs_trec_covid.pt'\n",
        "\n",
        "# Diretório onde vai salvando o modelo a cada época\n",
        "dir_modelos = '/content/drive/My Drive/IA368-DD_deep_learning_busca/Aula6-buscador-denso/modelos/'\n",
        "\n",
        "# Nome dos modelos e tokenizador. São esses modelos que serão carregados no início com o from_pretrained.\n",
        "# Se quiser iniciar um treinamento do 0, é necessário substituir por \"microsoft/MiniLM-L12-H384-uncased\"\n",
        "nome_modelo_query = '/content/drive/My Drive/IA368-DD_deep_learning_busca/Aula6-buscador-denso/modelos/final/query/' #nome_modelo_query = \"microsoft/MiniLM-L12-H384-uncased\"\n",
        "nome_modelo_doc = '/content/drive/My Drive/IA368-DD_deep_learning_busca/Aula6-buscador-denso/modelos/final/doc/' #nome_modelo_doc = \"microsoft/MiniLM-L12-H384-uncased\"\n",
        "nome_tokenizador = \"microsoft/MiniLM-L12-H384-uncased\"\n",
        "\n",
        "# Gerar e salvar arquivo de embeddings para os documentos usando os modelos da biblioteca Sequence Transformers:\n",
        "gerar_arquivo_doc_embeddings_all_mpnet_base_v2 = False\n",
        "arq_doc_embeddings_all_mpnet_base_v2 = '/content/drive/My Drive/IA368-DD_deep_learning_busca/Aula6-buscador-denso/docs_embeddings-all-mpnet-base-v2.pt'\n",
        "\n",
        "gerar_arquivo_doc_embeddings_all_MiniLM_L12_v2 = False\n",
        "arq_doc_embeddings_all_MiniLM_L12_v2 = '/content/drive/My Drive/IA368-DD_deep_learning_busca/Aula6-buscador-denso/docs_embeddings-all-MiniLM-L12-v2.pt'\n",
        "\n",
        "\n",
        "url_ms_marco_treinamento = \"https://storage.googleapis.com/unicamp-dl/ia368dd_2023s1/msmarco/msmarco_triples.train.tiny.tsv\"\n",
        "url_trec_covid = 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/trec-covid.zip'\n",
        "\n",
        "max_length = 256\n",
        "batch_size = 32\n",
        "epochs = 20\n",
        "lr = 2e-5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM6qeJFz-3Nl"
      },
      "source": [
        "### Instalação de libs e montagem do Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_dJy9vSFF4l",
        "outputId": "0ba1be86-6f33-4d98-a70b-a68d95a1328e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Já monta o drive, pois vamos usar o índice invertido da Aula 1 para usar o BM25 implementado também na aula 1\n",
        "# Além disso, é necessário para salvar/recuperar o modelo tunado\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install transformers datasets -q\n",
        "!pip install sentence-transformers -q\n",
        "!pip install pyserini -q\n",
        "!pip install faiss-gpu -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aq9LmOj4Fi4x"
      },
      "source": [
        "## Fine-tuning dos encoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJM5c2a5Fi_R"
      },
      "source": [
        "Cada linha do dataset de treino (MSMARCO-tiny) possui 3 campos: query, exemplo positivo, exemplo negativo. Vamos desconsiderar os exemplos negativos e usar apenas os positivos. Para uma dada query, usamos os exemplos positivos de outras queries como negativo para a query avaliada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-dEj2t4Fh6J",
        "outputId": "e7a75888-c466-4b29-84dc-be7aad5d3b18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total treinamento 9900 9900\n",
            "Total validação 1100 1100\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Só faz o download se ainda não tiver feito\n",
        "if not Path('./collections/msmarco_triples.train.tiny.tsv').is_file():\n",
        "  !wget {url_ms_marco_treinamento} -P collections # type: ignore\n",
        "\n",
        "# Lê usando pandas\n",
        "msmarco_df = pd.read_csv(\"collections/msmarco_triples.train.tiny.tsv\", sep='\\t', names=['query', 'relevante', 'nao_relevante'], header=None)\n",
        "msmarco_train_df, msmarco_val_df = train_test_split(msmarco_df, test_size=0.1, random_state=42)\n",
        "\n",
        "# Separa os conjuntos de treinamento e validação\n",
        "queries_train = msmarco_train_df['query'].tolist()\n",
        "docs_train = msmarco_train_df['relevante'].tolist()\n",
        "queries_val = msmarco_val_df['query'].tolist()\n",
        "docs_val = msmarco_val_df['relevante'].tolist()\n",
        "\n",
        "print('Total treinamento', len(queries_train), len(docs_train))\n",
        "print('Total validação', len(queries_val), len(docs_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7oebY_0_y-8"
      },
      "source": [
        "Define os datasets e dataloaders:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "xzu-esgqFOv2"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from torch.utils import data\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BatchEncoding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 512\n",
        "batch_size = 32\n",
        "epochs = 20\n",
        "lr = 2e-5"
      ],
      "metadata": {
        "id": "eAUmqB0V2GKH"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "b94xkxC-F5k2"
      },
      "outputs": [],
      "source": [
        "# Definição do Dataset\n",
        "class Dataset(data.Dataset):\n",
        "    # Recebe dois vetores de textos e um vetor de labels\n",
        "    def __init__(self, tokenizer, textos, labels, max_seq_length = max_length):\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.tokenizer = tokenizer\n",
        "        self.textos = textos\n",
        "        self.labels = labels\n",
        "        self.cache = {}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.textos)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        self.cache[str(idx)] = self.cache.get(str(idx),\n",
        "                   (self.tokenizer(self.textos[idx],\n",
        "                                  padding=True,\n",
        "                                  truncation=True,\n",
        "                                  max_length=self.max_seq_length\n",
        "                                  ),\n",
        "                    self.labels[idx])\n",
        "                   )\n",
        "        return self.cache[str(idx)]\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "pITFVK5LF-Ov"
      },
      "outputs": [],
      "source": [
        "bert_weights_name = 'neuralmind/bert-base-portuguese-cased'\n",
        "#bert_tokenizer = BertTokenizer.from_pretrained(bert_weights_name)\n",
        "#bert_model = BertModel.from_pretrained(bert_weights_name)\n",
        "\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(bert_weights_name)\n",
        "\n",
        "# Temos 2 datasets de cada tipo (train/val). Um pro encoder do documentos e outro pro encoder das queries\n",
        "# Datasets de treinamento\n",
        "dataset_queries_train = Dataset(tokenizer, queries_train)\n",
        "dataset_docs_train = Dataset(tokenizer, docs_train)\n",
        "\n",
        "# Datasets de validação\n",
        "dataset_queries_val = Dataset(tokenizer, queries_val)\n",
        "dataset_docs_val = Dataset(tokenizer, docs_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sure, in PyTorch, `Dataset` and `DataLoader` are essential components for working with batched data. Let's dive into them.\n",
        "\n",
        "**1. Dataset**\n",
        "\n",
        "In PyTorch, a dataset is represented by a subclass of the `torch.utils.data.Dataset` class. The dataset class is a way to provide an interface for accessing all the training or testing samples in your dataset. You need to override two methods when subclassing `Dataset`:\n",
        "\n",
        "* `__len__` so that len(dataset) returns the size of the dataset, and\n",
        "* `__getitem__` to support the indexing such that dataset[i] can be used to get the i-th sample.\n",
        "\n",
        "For example, let's say you have a list of images and their respective labels. Your dataset class would look something like this:\n",
        "\n",
        "```python\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = read_image(self.image_paths[idx])  # Some function to read image from path\n",
        "        label = self.labels[idx]\n",
        "        return image, label\n",
        "```\n",
        "\n",
        "**2. DataLoader**\n",
        "\n",
        "While the `Dataset` class is used to provide an interface for accessing the data, the `DataLoader` is used to load the data in batches during training. This is beneficial because it can significantly reduce the amount of memory used (especially for large datasets).\n",
        "\n",
        "The `DataLoader` takes in a dataset and a variety of optional parameters. One of the most important ones is `batch_size`, which determines how many samples per batch to load. Other parameters like `shuffle` determine whether data is served in a random order. Here's an example of using a `DataLoader` with the previous `MyDataset` class:\n",
        "\n",
        "```python\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataset = MyDataset(image_paths, labels)\n",
        "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "for batch in dataloader:\n",
        "    images, labels = batch\n",
        "    # Do something with these batches of images and labels...\n",
        "```\n",
        "\n",
        "In this code, `dataloader` is an iterator — we can loop over it and fetch batches of images and labels.\n",
        "\n",
        "Additionally, `DataLoader` is typically used in conjunction with the `Dataset` class. The `Dataset` retrieves our dataset's items, while the `DataLoader` wraps an iterable around the `Dataset` to enable easy access to the samples.\n",
        "\n",
        "**3. collate_fn**\n",
        "\n",
        "The `collate_fn` is a function that the `DataLoader` applies to a list of samples from the `Dataset` to form a batch. The `collate_fn` is necessary because the number of samples in different items in the dataset may not be the same. So, we need a function to correctly collate these samples into a batch.\n",
        "\n",
        "The default `collate_fn` should work fine for most use cases. However, if you have a complex structure of the data, or the data is not a tensor, you might need to write your own `collate_fn`.\n",
        "\n",
        "**Note:** The examples provided in this explanation are relatively basic. Depending on the complexity of your dataset and what you want to do with your data, your `Dataset` and `DataLoader` implementations could become more complex. For example, you may need to transform your data (like resizing images, or tokenizing text), in which case these transformations would typically be implemented in your `Dataset` class.\n"
      ],
      "metadata": {
        "id": "Kn4d9OheDW3A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "fC1YppfOGXur"
      },
      "outputs": [],
      "source": [
        "# Create datasets\n",
        "dataset_queries_train = Dataset(tokenizer, queries_train, positive_ex_train)\n",
        "dataset_docs_train = Dataset(tokenizer, docs_train, positive_ex_train)\n",
        "\n",
        "dataset_queries_val = Dataset(tokenizer, queries_val, positive_ex_val)\n",
        "dataset_docs_val = Dataset(tokenizer, docs_val, positive_ex_val)\n",
        "\n",
        "\n",
        "# Dataloaders para os datasets\n",
        "\n",
        "#collate_fn = lambda batch: BatchEncoding(tokenizer.pad(batch, return_tensors='pt'))\n",
        "def collate_fn(batch):\n",
        "    #print('Dentro de collate_fn')\n",
        "    #print(BatchEncoding(tokenizer.pad(batch, return_tensors='pt')))\n",
        "    return BatchEncoding(tokenizer.pad(batch, return_tensors='pt'))\n",
        "\n",
        "# collate function that also handles the labels\n",
        "def collate_fn(batch):\n",
        "    inputs = [item[0] for item in batch]\n",
        "    labels = [item[1] for item in batch]\n",
        "    return BatchEncoding(tokenizer.pad(inputs, return_tensors='pt')), torch.tensor(labels)\n",
        "\n",
        "\n",
        "# Create dataloaders\n",
        "dataloader_queries_train = DataLoader(dataset_queries_train, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "dataloader_docs_train = DataLoader(dataset_docs_train, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "dataloader_queries_val = DataLoader(dataset_queries_val, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "dataloader_docs_val = DataLoader(dataset_docs_val, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "\n",
        "\n",
        "# Adapt your DataLoader object\n",
        "class PositiveExDataLoader:\n",
        "    def __init__(self, dataloader, positive_ex):\n",
        "        self.dataloader = dataloader\n",
        "        self.positive_ex = positive_ex\n",
        "\n",
        "    def __iter__(self):\n",
        "        for batch, pos_ex in zip(self.dataloader, self.positive_ex):\n",
        "            yield batch, pos_ex\n",
        "    def __len__(self):\n",
        "        return len(self.dataloader)\n",
        "# Wrap your original dataloaders\n",
        "dataloader_queries_train = PositiveExDataLoader(dataloader_queries_train, positive_ex_train)\n",
        "dataloader_docs_train = PositiveExDataLoader(dataloader_docs_train, positive_ex_train)\n",
        "\n",
        "dataloader_queries_val = PositiveExDataLoader(dataloader_queries_val, positive_ex_val)\n",
        "dataloader_docs_val = PositiveExDataLoader(dataloader_docs_val, positive_ex_val)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGouSrQgF-sn"
      },
      "source": [
        "Carrega os modelos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "FpWhDxhFGh2G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53b304d1-7b96-4ddf-d20e-9475b0d10c2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# Agora vamos carregar dois modelos:\n",
        "import torch\n",
        "from transformers import AutoModel\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Se tiver que treinar os modelos, abre\n",
        "model_query = AutoModel.from_pretrained(bert_weights_name).to(device)\n",
        "model_doc = AutoModel.from_pretrained(bert_weights_name).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xinVUAMdGnoa"
      },
      "source": [
        "Define função pro cálculo da loss (modifiquei essa função para trabalhar com um vetor indicando quais pares são relevantes e quais não são:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "nE_Kk3ZeGp2L"
      },
      "outputs": [],
      "source": [
        "# Essa função já considera o resultado via batchs:\n",
        "def compute_loss_com_gradiente(model_query, tokenized_queries, model_doc, tokenized_docs, positive_ex):\n",
        "    outputs_queries = model_query(**tokenized_queries.to(device))\n",
        "    outputs_docs    = model_doc(**tokenized_docs.to(device))\n",
        "\n",
        "    # Extrai a última camada oculta associada ao token [CLS]\n",
        "    tcls_queries = outputs_queries.last_hidden_state[:, 0, :]\n",
        "    tcls_docs    = outputs_docs.last_hidden_state[:, 0, :]\n",
        "\n",
        "    # Normaliza os tensores\n",
        "    #tcls_queries = tcls_queries / torch.norm(tcls_queries, dim=1, keepdim=True)\n",
        "    #tcls_docs = tcls_docs / torch.norm(tcls_docs, dim=1, keepdim=True)\n",
        "\n",
        "    # Agora é necessário calcular a loss. Para isso, o primeiro passo é\n",
        "    # calcular a similaridade entre uma query e documento (sim(q, d))\n",
        "    similaridade = torch.matmul(tcls_queries, torch.transpose(tcls_docs, 0, 1))\n",
        "\n",
        "    # Calcula a exponencial da similaridade\n",
        "    exp_sim = torch.exp(similaridade)\n",
        "\n",
        "    # Calcula a loss\n",
        "    # We are now considering only the positive examples (where positive_ex is 1)\n",
        "    positive_exp_sim = exp_sim * positive_ex.to(device)\n",
        "    soma_linhas = positive_exp_sim.sum(dim=1)\n",
        "    diagonal = torch.diag(positive_exp_sim)\n",
        "    log_loss = -1* torch.log(diagonal/soma_linhas)\n",
        "\n",
        "    loss = torch.mean(log_loss)\n",
        "    return loss\n",
        "\n",
        "def compute_loss_sem_gradiente(model_query, tokenized_queries, model_doc, tokenized_docs):\n",
        "    with torch.no_grad():\n",
        "        return compute_loss_com_gradiente(model_query, tokenized_queries, model_doc, tokenized_docs)\n",
        "\n",
        "def compute_loss_dataloaders(model_query, dataloader_query, model_doc, dataloader_docs):\n",
        "    loss = 0\n",
        "    n_batches = 0\n",
        "    for batch_query, batch_docs in zip(dataloader_query, dataloader_docs):\n",
        "        loss = loss + compute_loss_sem_gradiente(model_query, batch_query, model_doc, batch_docs)\n",
        "        n_batches += 1\n",
        "    return loss/n_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xTW39DGTuXY",
        "outputId": "fdac0113-0d0f-4ea1-b44a-a725cf8db11c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss de treinamento: 0.0008150112116709352\n",
            "Loss de validação: 0.04865604639053345\n",
            "CPU times: user 41.7 s, sys: 119 ms, total: 41.8 s\n",
            "Wall time: 42.1 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Só pra medir o tempo que ele demora para calcular a loss em todo o dataset de treinamento\n",
        "model_query.eval()\n",
        "model_doc.eval()\n",
        "print(f'Loss de treinamento: {compute_loss_dataloaders(model_query, dataloader_queries_train, model_doc, dataloader_docs_train)}')\n",
        "print(f'Loss de validação: {compute_loss_dataloaders(model_query, dataloader_queries_val, model_doc, dataloader_docs_val)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KST5dRqWGxff"
      },
      "source": [
        "Agora treina os dois encoders simulatenamente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457,
          "referenced_widgets": [
            "c5fea8f8e554482e80851976e1244ff2",
            "2ab7318319c84d5095b3da5a0cbc2da1",
            "010c8436910a4313b1e6ca604a1a2003",
            "4f12d6ca46e54c5097c6d43a1a2b88da",
            "cccab72ecfa44b86b018e0701c6a8f67",
            "ae1c80902daa4e24a7c8d299f3127aaf",
            "99b8bd8d26aa4e52b3087830f211078e",
            "9cc0374005ca45ea91ca03728b5f145d",
            "fad601cd79c848b7b9ce577c89e3b896",
            "ce2323e21be34a0fbaf1fe30f44f5cb9",
            "501772ffcefa4e29b3ed32be38ccfc35"
          ]
        },
        "id": "JrzMILg4GxDz",
        "outputId": "a0d17b32-15dd-4a30-abb6-370389e46290"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epochs:   0%|          | 0/20 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c5fea8f8e554482e80851976e1244ff2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-f092cdada664>\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_ex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositive_ex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_ex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-cf6a6f77831e>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         self.cache[str(idx)] = self.cache.get(str(idx),\n\u001b[0;32m---> 16\u001b[0;31m                    (self.tokenizer(self.textos[idx],\n\u001b[0m\u001b[1;32m     17\u001b[0m                                   \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                                   \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import get_linear_schedule_with_warmup, get_cosine_with_hard_restarts_schedule_with_warmup, AdamW\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "treinar_e_salvar_modelos  = True\n",
        "\n",
        "\n",
        "if treinar_e_salvar_modelos:\n",
        "  # Training loop\n",
        "  optimizer_query = AdamW(model_query.parameters(), lr=lr)\n",
        "  optimizer_doc = AdamW(model_doc.parameters(), lr=lr)\n",
        "\n",
        "  num_training_steps = epochs * len(dataloader_queries_train)\n",
        "  num_warmup_steps = int(num_training_steps * 0.1)\n",
        "\n",
        "  # get_linear_schedule_with_warmup get_cosine_with_hard_restarts_schedule_with_warmup\n",
        "  scheduler_query = get_cosine_with_hard_restarts_schedule_with_warmup(optimizer_query, num_warmup_steps, num_training_steps)\n",
        "  scheduler_doc = get_cosine_with_hard_restarts_schedule_with_warmup(optimizer_doc, num_warmup_steps, num_training_steps)\n",
        "\n",
        "  for epoch in tqdm(range(epochs), desc='Epochs'):\n",
        "      model_query.train()\n",
        "      model_doc.train()\n",
        "\n",
        "      train_losses = []\n",
        "      for (batch_query, positive_ex_query), (batch_docs, positive_ex_docs) in tqdm(list(zip(dataloader_queries_train, dataloader_docs_train)), mininterval=0.5, desc='Train', disable=False):\n",
        "        optimizer_query.zero_grad()\n",
        "        optimizer_doc.zero_grad()\n",
        "\n",
        "        # Ensure positive_ex_query and positive_ex_docs are tensors and on the same device as your models\n",
        "        positive_ex_query = torch.tensor(positive_ex_query).to(device)\n",
        "        positive_ex_docs = torch.tensor(positive_ex_docs).to(device)\n",
        "\n",
        "        loss = compute_loss_com_gradiente(model_query, batch_query, model_doc, batch_docs, positive_ex_query, positive_ex_docs)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer_query.step()\n",
        "        optimizer_doc.step()\n",
        "\n",
        "        scheduler_query.step()\n",
        "        scheduler_doc.step()\n",
        "\n",
        "\n",
        "      model_query.save_pretrained(f'{dir_modelos}{epoch}/query/')\n",
        "      model_doc.save_pretrained(f'{dir_modelos}{epoch}/doc/')\n",
        "\n",
        "      model_query.eval()\n",
        "      model_doc.eval()\n",
        "\n",
        "      print(f'Loss de treinamento {epoch}: {compute_loss_dataloaders(model_query, dataloader_queries_train, model_doc, dataloader_docs_train)}')\n",
        "      print(f'Loss de validação {epoch}: {compute_loss_dataloaders(model_query, dataloader_queries_val, model_doc, dataloader_docs_val)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sun4lP8KCAvs"
      },
      "source": [
        "## Pesquisa completa no TREC-COVID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kLOVQsjj25U"
      },
      "source": [
        "Treinado o modelo, agora vamos aplicá-lo ao TREC-COVID:.\n",
        "\n",
        "Coloca os modelos em modo eval:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNXUml6aRki7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77434a83-1a74-42ff-d34b-19813543c68d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 384, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 384)\n",
              "    (token_type_embeddings): Embedding(2, 384)\n",
              "    (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
              "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
              "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
              "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "model_query.eval()\n",
        "model_doc.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Baixa o trec-covid:"
      ],
      "metadata": {
        "id": "qSzSdHkpqA7E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3t9H9Dyj8F0"
      },
      "outputs": [],
      "source": [
        "if not Path('./collections/trec-covid.zip').is_file():\n",
        "  !wget {url_trec_covid} -P collections # type: ignore\n",
        "  !unzip -o collections/trec-covid.zip -d ./collections # type: ignore\n",
        "\n",
        "# Converte o qrels que veio no trec-covid.zip pra o formato esperado:\n",
        "with open('./collections/trec-covid/qrels/test.tsv', 'r') as fin:\n",
        "  data = fin.read().splitlines(True)\n",
        "with open('./collections/trec-covid/qrels/test_corrigido.tsv', 'w') as fout:\n",
        "  for linha in data[1:]:\n",
        "    campos = linha.split()\n",
        "    fout.write(f'{campos[0]}\\t0\\t{campos[1]}\\t{campos[2]}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Carrega as queries e os documentos:"
      ],
      "metadata": {
        "id": "D-d2PEMXqEWA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "755HjJD1k_x-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d92c1e8e-1f80-496a-ff19-3902c5ace133"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processado 0 documentos\n",
            "Processado 10000 documentos\n",
            "Processado 20000 documentos\n",
            "Processado 30000 documentos\n",
            "Processado 40000 documentos\n",
            "Processado 50000 documentos\n",
            "Processado 60000 documentos\n",
            "Processado 70000 documentos\n",
            "Processado 80000 documentos\n",
            "Processado 90000 documentos\n",
            "Processado 100000 documentos\n",
            "Processado 110000 documentos\n",
            "Processado 120000 documentos\n",
            "Processado 130000 documentos\n",
            "Processado 140000 documentos\n",
            "Processado 150000 documentos\n",
            "Processado 160000 documentos\n",
            "Processado 170000 documentos\n",
            "CPU times: user 1.83 s, sys: 102 ms, total: 1.93 s\n",
            "Wall time: 1.9 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import json\n",
        "\n",
        "def carrega_corpus_trec_covid():\n",
        "  retorno = []\n",
        "  with open('./collections/trec-covid/corpus.jsonl') as corpus:\n",
        "    for i, line in enumerate(corpus):\n",
        "      doc = json.loads(line)\n",
        "      #retorno.append({\n",
        "      #    'id': doc['_id'],\n",
        "      #    'doc': f\"{doc['title']} {doc['text']}\"\n",
        "      #})\n",
        "      retorno.append(\n",
        "          (doc['_id'], f\"{doc['title']} {doc['text']}\")\n",
        "      )\n",
        "      if (i % 10000 == 0):\n",
        "        print(f'Processado {i} documentos')\n",
        "    return retorno\n",
        "\n",
        "def carrega_queries_trec_covid():\n",
        "  retorno = []\n",
        "  with open('./collections/trec-covid/queries.jsonl') as queries:\n",
        "    for line in queries:\n",
        "      query = json.loads(line)\n",
        "      # Faz apenas uma pequena tradução de _id para id e text para texto\n",
        "      retorno.append({'id': query['_id'], 'texto': query['text']})\n",
        "  return retorno\n",
        "\n",
        "queries_trec_covid = carrega_queries_trec_covid()\n",
        "corpus_trec_covid = carrega_corpus_trec_covid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLTscG-JCLy7"
      },
      "source": [
        "A variável corpus_trec_covid contém os ids e os textos. Agora é necessário carregar a representação vetorial desses textos. Isso será feito gerando a matriz matriz_docs_trec_covid:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNsy7URQm4ws",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1e87ecc-be2b-47b3-fcb3-0bec1ffeb920"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([171332, 384])\n",
            "CPU times: user 215 ms, sys: 333 ms, total: 549 ms\n",
            "Wall time: 459 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "ids_trec_covid, textos_trec_covid = zip(*corpus_trec_covid)\n",
        "\n",
        "matriz_docs_trec_covid = None\n",
        "#textos_trec_covid = textos_trec_covid[0:20000]\n",
        "dataset_docs_trec_covid = Dataset(tokenizer, textos_trec_covid)\n",
        "dataloader_docs_trec_covid = DataLoader(dataset_docs_trec_covid, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "if gerar_e_salvar_matriz_docs_trec_covid:\n",
        "  with torch.no_grad():\n",
        "    for batch in tqdm(dataloader_docs_trec_covid, mininterval=0.5, desc='Convertendo documentos trec-covid', disable=False):\n",
        "      outputs_docs = model_doc(**batch.to(device))\n",
        "      tcls_docs    = outputs_docs.last_hidden_state[:, 0, :]\n",
        "      #tcls_docs    = tcls_docs / torch.norm(tcls_docs, dim=1, keepdim=True)\n",
        "      # Monta a matriz de documentos na CPU\n",
        "      tcls_docs = tcls_docs.to(\"cpu\")\n",
        "\n",
        "      if matriz_docs_trec_covid is None:\n",
        "        matriz_docs_trec_covid = tcls_docs\n",
        "      else:\n",
        "        matriz_docs_trec_covid = torch.cat( (matriz_docs_trec_covid, tcls_docs), dim=0)\n",
        "\n",
        "  # Agora volta a matriz pra GPU pq ela cabe lá, não sei pq estava estourando a memória antes...\n",
        "  matriz_docs_trec_covid = matriz_docs_trec_covid.to(device)\n",
        "  torch.save(matriz_docs_trec_covid, arquivo_matriz_docs_trec_covid)\n",
        "else:\n",
        "  matriz_docs_trec_covid = torch.load(arquivo_matriz_docs_trec_covid).to(device)\n",
        "\n",
        "print(matriz_docs_trec_covid.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBFvzp3BrD0X"
      },
      "source": [
        "Agora vamos definir um método pra calcular a representação vetorial da query e para fazer a pesquisa na base de dados. O método de pesquisa só retorna o vetor de score pareado com as ids contidas na variável ids_trec_covid:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIQ2VNrdq_kH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02c7c679-e70b-4b5d-87c7-4e8bc790a093"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([384])\n"
          ]
        }
      ],
      "source": [
        "def get_vetor_query(query):\n",
        "  query_tokenizada = tokenizer(query, padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
        "\n",
        "  with torch.no_grad():\n",
        "    output_query = model_query(**query_tokenizada.to(device))\n",
        "    tcls_query    = output_query.last_hidden_state[:, 0, :]\n",
        "    # tcls_query    = tcls_query / torch.norm(tcls_query, dim=1, keepdim=True)\n",
        "\n",
        "  return tcls_query[0]\n",
        "\n",
        "vetor_query = get_vetor_query('what is this?')\n",
        "\n",
        "print(vetor_query.size())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1F9iF_6KsB-O"
      },
      "outputs": [],
      "source": [
        "def calcula_score_documentos_para_a_query(matriz_docs, query):\n",
        "  vetor_query = get_vetor_query(query)\n",
        "  score = torch.matmul(matriz_docs, vetor_query)\n",
        "\n",
        "  return score\n",
        "\n",
        "def pesquisa_query_e_retorna_n_primeiros_docs(matriz_docs, ids_docs_na_matriz, query, n=1000):\n",
        "  # Calcula o score\n",
        "  score = calcula_score_documentos_para_a_query(matriz_docs, query)\n",
        "  # Ordena\n",
        "  sorted_score, indices_score = torch.sort(score, descending=True)\n",
        "  # Pega só os n primeiros\n",
        "  sorted_score = sorted_score[0:n]\n",
        "  indices_score = indices_score[0:n]\n",
        "  # Extrai os ids dos documentos\n",
        "  ids_docs = [ids_docs_na_matriz[i] for i in indices_score]\n",
        "\n",
        "  return zip(ids_docs, sorted_score)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora roda todas as queries para avaliação..."
      ],
      "metadata": {
        "id": "8B56kNR_wfCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Roda todas as queries\n",
        "def run_all_queries(file):\n",
        "  print('Carregando as queries do arquivo queries.jsonl...\\n')\n",
        "  queries_trec_covid = carrega_queries_trec_covid()\n",
        "\n",
        "  print(f'Total de queries que serão avaliadas: {len(queries_trec_covid)}')\n",
        "  cnt = 0\n",
        "  with open(file, 'w') as runfile:\n",
        "    for query in queries_trec_covid:\n",
        "      id_query = query['id']\n",
        "      texto = query['texto']\n",
        "\n",
        "      if cnt % 5 == 0:\n",
        "        print(f'{cnt} queries completadas')\n",
        "\n",
        "      # Pega os primeiros 1000 resultados\n",
        "      docs_score = pesquisa_query_e_retorna_n_primeiros_docs(matriz_docs_trec_covid, ids_trec_covid, texto, n=1000)\n",
        "\n",
        "      i = 0\n",
        "      for id_doc, score in docs_score:\n",
        "        _ = runfile.write('{} Q0 {} {} {:.6f} Pesquisa_densa\\n'.format(id_query, id_doc, i+1, float(score)))\n",
        "        i += 1\n",
        "\n",
        "      cnt += 1\n",
        "      # break # Pra testar, gera só a primeira query\n",
        "    print(f'{cnt} queries completadas')\n"
      ],
      "metadata": {
        "id": "C6PFulWMsgX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "E calcula o nDCG@10:"
      ],
      "metadata": {
        "id": "5lVWkSeBwiNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_all_queries('run-pesquisa-densa.txt')\n",
        "\n",
        "!python -m pyserini.eval.trec_eval -c -m ndcg_cut.10 collections/trec-covid/qrels/test_corrigido.tsv run-pesquisa-densa.txt #type: ignore"
      ],
      "metadata": {
        "id": "wuDvWPs8wYCX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b8b432e-d669-400d-b0ba-6c1e79543175"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregando as queries do arquivo queries.jsonl...\n",
            "\n",
            "Total de queries que serão avaliadas: 50\n",
            "0 queries completadas\n",
            "5 queries completadas\n",
            "10 queries completadas\n",
            "15 queries completadas\n",
            "20 queries completadas\n",
            "25 queries completadas\n",
            "30 queries completadas\n",
            "35 queries completadas\n",
            "40 queries completadas\n",
            "45 queries completadas\n",
            "50 queries completadas\n",
            "Downloading https://search.maven.org/remotecontent?filepath=uk/ac/gla/dcs/terrierteam/jtreceval/0.0.5/jtreceval-0.0.5-jar-with-dependencies.jar to /root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar...\n",
            "/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar already exists!\n",
            "Skipping download.\n",
            "Running command: ['java', '-jar', '/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar', '-c', '-m', 'ndcg_cut.10', 'collections/trec-covid/qrels/test_corrigido.tsv', 'run-pesquisa-densa.txt']\n",
            "Results:\n",
            "ndcg_cut_10           \tall\t0.3322\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pesquisa aproximada (usando k-means)"
      ],
      "metadata": {
        "id": "zX2vUHn-KKQx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos pegar a matriz de documentos e criar 10 clusters para eles. E treinar um kmeans:"
      ],
      "metadata": {
        "id": "1mNTBy1rjAlv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "# Primeiro vamos transformar a matriz de documentos para uma matriz numpy:\n",
        "matriz_docs_np = matriz_docs_trec_covid.to('cpu').numpy()\n",
        "matriz_docs_trec_covid.to(device)\n",
        "\n",
        "# Cálculo dos centróides:\n",
        "n_clusters = 10\n",
        "kmeans = KMeans(n_clusters=n_clusters, init='k-means++', max_iter=300, n_init=10, random_state=42)\n",
        "classe_doc = kmeans.fit_predict(matriz_docs_np)\n"
      ],
      "metadata": {
        "id": "KgxGXwvvLOtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora vamos separar os índices dos documentos nesses 10 clusters e deixar em um dict só pra facilitar as coisas depois:"
      ],
      "metadata": {
        "id": "MlNGoj0Uj7Hs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "indices_por_cluster = {}\n",
        "\n",
        "for classe in range(0, n_clusters):\n",
        "  indices_por_cluster[classe] = {\n",
        "      'indice': [indice for indice, classe_doc in enumerate(classe_doc) if classe == classe_doc],\n",
        "      'ids_doc': [ids_trec_covid[indice] for indice, classe_doc in enumerate(classe_doc) if classe == classe_doc]\n",
        "  }\n",
        "  print(f'Total docs na classe {classe} = {len(indices_por_cluster[classe][\"indice\"])}')\n",
        "\n",
        "print(indices_por_cluster[0]['indice'][0:3])\n",
        "print(indices_por_cluster[0]['ids_doc'][0:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZ9NFEmdiveS",
        "outputId": "b09781bf-0ed7-4be7-a1da-f341d2bb5e3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total docs na classe 0 = 28914\n",
            "Total docs na classe 1 = 12221\n",
            "Total docs na classe 2 = 24366\n",
            "Total docs na classe 3 = 22542\n",
            "Total docs na classe 4 = 11836\n",
            "Total docs na classe 5 = 11859\n",
            "Total docs na classe 6 = 22447\n",
            "Total docs na classe 7 = 8259\n",
            "Total docs na classe 8 = 15198\n",
            "Total docs na classe 9 = 13690\n",
            "[2, 4, 5]\n",
            "['ejv2xln0', '9785vg6d', 'zjufx4fo']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos definir um método pra, dada uma query (um texto), retornar qual é a classe de documentos que está mais próxima dessa query. Ou seja, esse método extrai o vetor que representa a query e calcula a distância entre esse vetor e o vetor do centro de cada cluster:"
      ],
      "metadata": {
        "id": "75hiWt7Wk8Mf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_classe_mais_proxima(query):\n",
        "  vetor_query = get_vetor_query(query).to('cpu').numpy()\n",
        "\n",
        "  classe_mais_proxima = -1\n",
        "  dist_mais_proxima = 1e10\n",
        "  for classe, vetor_classe in enumerate(kmeans.cluster_centers_):\n",
        "    dist = np.linalg.norm(vetor_query - vetor_classe)\n",
        "    if dist < dist_mais_proxima:\n",
        "      dist_mais_proxima = dist\n",
        "      classe_mais_proxima = classe\n",
        "\n",
        "  return classe_mais_proxima\n",
        "\n",
        "get_classe_mais_proxima('What is this?')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJLK2TH2lEvO",
        "outputId": "63eeced8-8c49-464e-900a-be4fff5afd99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora um método que faz a pesquisa apenas nos documentos que estão no cluster mais próximo.\n",
        "\n",
        "Como já separamos os ids dos documentos por cluster dentro de indices_por_cluster[classe_mais_proxima]['ids_doc'], o que precisamos agora é só filtrar a matriz matriz_docs_trec_covid pra considerar apenas os documentos equivalentes a indices_por_cluster[classe_mais_proxima]['ids_doc']. Feito isso, basta passar pro método de pesquisa geral:"
      ],
      "metadata": {
        "id": "OsEQAkP_y3XF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pesquisa_query_na_classe_mais_proxima_e_retorna_n_primeiros_docs(query, n=1000):\n",
        "  classe_mais_proxima = get_classe_mais_proxima(query)\n",
        "  indices_na_classe = indices_por_cluster[classe_mais_proxima]['indice']\n",
        "\n",
        "  matriz_filtrada = matriz_docs_trec_covid[torch.LongTensor(indices_na_classe).to(torch.int)]\n",
        "  ids_docs_filtrados = indices_por_cluster[classe_mais_proxima]['ids_doc']\n",
        "\n",
        "  return pesquisa_query_e_retorna_n_primeiros_docs(matriz_filtrada, ids_docs_filtrados, query, n)\n"
      ],
      "metadata": {
        "id": "ixN60X_DmsUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "for i in pesquisa_query_na_classe_mais_proxima_e_retorna_n_primeiros_docs('teste', 10):\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-QVU9NWn2hH",
        "outputId": "16d40b64-9f92-4f38-f5f9-2dfcf7ee5779"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('v6nnnzhm', tensor(61.4928, device='cuda:0'))\n",
            "('crtvxdn6', tensor(58.7026, device='cuda:0'))\n",
            "('iryg7xw0', tensor(57.4697, device='cuda:0'))\n",
            "('kyae3q05', tensor(57.1972, device='cuda:0'))\n",
            "('1qhbucb0', tensor(57.0405, device='cuda:0'))\n",
            "('zlx4etkg', tensor(55.8098, device='cuda:0'))\n",
            "('1x3da6ko', tensor(55.6850, device='cuda:0'))\n",
            "('4oabmb1a', tensor(55.6653, device='cuda:0'))\n",
            "('4cpiemq5', tensor(55.3849, device='cuda:0'))\n",
            "('g6sfaa2m', tensor(55.3795, device='cuda:0'))\n",
            "CPU times: user 43.3 ms, sys: 3.05 ms, total: 46.4 ms\n",
            "Wall time: 44.6 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como o run_all_queries tinha sido definido pesquisando apenas em todos os documentos, vamos replicar ele aqui mas agora mandando pesquisar apenas na classe mais próxima:"
      ],
      "metadata": {
        "id": "Q1aKFYzNzOqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Roda todas as queries\n",
        "def run_all_queries_classe_proxima(file):\n",
        "  print('Carregando as queries do arquivo queries.jsonl...\\n')\n",
        "  queries_trec_covid = carrega_queries_trec_covid()\n",
        "\n",
        "  print(f'Total de queries que serão avaliadas: {len(queries_trec_covid)}')\n",
        "  cnt = 0\n",
        "  with open(file, 'w') as runfile:\n",
        "    for query in queries_trec_covid:\n",
        "      id_query = query['id']\n",
        "      texto = query['texto']\n",
        "\n",
        "      if cnt % 5 == 0:\n",
        "        print(f'{cnt} queries completadas')\n",
        "\n",
        "      # Pega os primeiros 1000 resultados\n",
        "      docs_score = pesquisa_query_na_classe_mais_proxima_e_retorna_n_primeiros_docs(texto, n=1000)\n",
        "\n",
        "      i = 0\n",
        "      for id_doc, score in docs_score:\n",
        "        _ = runfile.write('{} Q0 {} {} {:.6f} Pesquisa_densa\\n'.format(id_query, id_doc, i+1, float(score)))\n",
        "        i += 1\n",
        "\n",
        "      cnt += 1\n",
        "      # break # Pra testar, gera só a primeira query\n",
        "    print(f'{cnt} queries completadas')\n"
      ],
      "metadata": {
        "id": "8pyumrgunpo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por fim, roda a pesquisa e checa o nDCG@10:"
      ],
      "metadata": {
        "id": "r_zCDCbBz2Ik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_all_queries_classe_proxima('run-pesquisa-densa-classe-proxima.txt')\n",
        "\n",
        "!python -m pyserini.eval.trec_eval -c -m ndcg_cut.10 collections/trec-covid/qrels/test_corrigido.tsv run-pesquisa-densa-classe-proxima.txt #type: ignore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJItv_npnwYT",
        "outputId": "6a3f2998-3733-417f-944d-59367605bf15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregando as queries do arquivo queries.jsonl...\n",
            "\n",
            "Total de queries que serão avaliadas: 50\n",
            "0 queries completadas\n",
            "5 queries completadas\n",
            "10 queries completadas\n",
            "15 queries completadas\n",
            "20 queries completadas\n",
            "25 queries completadas\n",
            "30 queries completadas\n",
            "35 queries completadas\n",
            "40 queries completadas\n",
            "45 queries completadas\n",
            "50 queries completadas\n",
            "Downloading https://search.maven.org/remotecontent?filepath=uk/ac/gla/dcs/terrierteam/jtreceval/0.0.5/jtreceval-0.0.5-jar-with-dependencies.jar to /root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar...\n",
            "/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar already exists!\n",
            "Skipping download.\n",
            "Running command: ['java', '-jar', '/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar', '-c', '-m', 'ndcg_cut.10', 'collections/trec-covid/qrels/test_corrigido.tsv', 'run-pesquisa-densa-classe-proxima.txt']\n",
            "Results:\n",
            "ndcg_cut_10           \tall\t0.2991\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usando sentence transformers"
      ],
      "metadata": {
        "id": "JwUf5ldnwemk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "model = SentenceTransformer('all-mpnet-base-v2').to(device)\n",
        "\n",
        "if gerar_arquivo_doc_embeddings_all_mpnet_base_v2:\n",
        "  docs_embeddings = model.encode(textos_trec_covid)\n",
        "  torch.save(docs_embeddings, arq_doc_embeddings_all_mpnet_base_v2)\n",
        "else:\n",
        "  docs_embeddings = torch.load(arq_doc_embeddings_all_mpnet_base_v2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sUaIc6Twg_V",
        "outputId": "d2cc2c93-5577-4ea6-d8c7-4b8569db621b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 5.13 s, sys: 2.45 s, total: 7.58 s\n",
            "Wall time: 7.59 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calcula_score_documentos_para_a_query_sentence_transformers(model, docs_embeddings, query):\n",
        "  query_embedding = model.encode(query)\n",
        "  return util.dot_score(query_embedding, docs_embeddings)\n",
        "\n",
        "def pesquisa_query_e_retorna_n_primeiros_docs_sentence_transformers(model, docs_embeddings, query, n=1000):\n",
        "  # Calcula o score\n",
        "  score = calcula_score_documentos_para_a_query_sentence_transformers(model, docs_embeddings, query)\n",
        "  # Ordena\n",
        "  sorted_score, indices_score = torch.sort(score, descending=True)\n",
        "  # Pega só os n primeiros\n",
        "  sorted_score = sorted_score[0, 0:n]\n",
        "  indices_score = indices_score[0, 0:n]\n",
        "  # Extrai os ids dos documentos\n",
        "  ids_docs = [ids_trec_covid[i] for i in indices_score]\n",
        "\n",
        "  return zip(ids_docs, sorted_score)\n",
        "\n",
        "# Roda todas as queries\n",
        "def run_all_queries_sentence_transformer(model, docs_embeddings, file):\n",
        "  print('Carregando as queries do arquivo queries.jsonl...\\n')\n",
        "  queries_trec_covid = carrega_queries_trec_covid()\n",
        "\n",
        "  print(f'Total de queries que serão avaliadas: {len(queries_trec_covid)}')\n",
        "  cnt = 0\n",
        "  with open(file, 'w') as runfile:\n",
        "    for query in queries_trec_covid:\n",
        "      id_query = query['id']\n",
        "      texto = query['texto']\n",
        "\n",
        "      if cnt % 5 == 0:\n",
        "        print(f'{cnt} queries completadas')\n",
        "\n",
        "      # Pega os primeiros 1000 resultados\n",
        "      docs_score = pesquisa_query_e_retorna_n_primeiros_docs_sentence_transformers(model, docs_embeddings, texto, n=1000)\n",
        "\n",
        "      i = 0\n",
        "      for id_doc, score in docs_score:\n",
        "        _ = runfile.write('{} Q0 {} {} {:.6f} Pesquisa_densa\\n'.format(id_query, id_doc, i+1, float(score)))\n",
        "        i += 1\n",
        "\n",
        "      cnt += 1\n",
        "      # break # Pra testar, gera só a primeira query\n",
        "    print(f'{cnt} queries completadas')\n",
        "\n",
        "run_all_queries_sentence_transformer(model, docs_embeddings, 'run-pesquisa-densa-sentence-transformer-all-mpnet-base-v2.txt')\n",
        "!python -m pyserini.eval.trec_eval -c -m ndcg_cut.10 collections/trec-covid/qrels/test_corrigido.tsv run-pesquisa-densa-sentence-transformer-all-mpnet-base-v2.txt #type: ignore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2t_mKrUxs3_",
        "outputId": "b58e72be-54cc-4780-c06e-4a9df60b82e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregando as queries do arquivo queries.jsonl...\n",
            "\n",
            "Total de queries que serão avaliadas: 50\n",
            "0 queries completadas\n",
            "5 queries completadas\n",
            "10 queries completadas\n",
            "15 queries completadas\n",
            "20 queries completadas\n",
            "25 queries completadas\n",
            "30 queries completadas\n",
            "35 queries completadas\n",
            "40 queries completadas\n",
            "45 queries completadas\n",
            "50 queries completadas\n",
            "Downloading https://search.maven.org/remotecontent?filepath=uk/ac/gla/dcs/terrierteam/jtreceval/0.0.5/jtreceval-0.0.5-jar-with-dependencies.jar to /root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar...\n",
            "/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar already exists!\n",
            "Skipping download.\n",
            "Running command: ['java', '-jar', '/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar', '-c', '-m', 'ndcg_cut.10', 'collections/trec-covid/qrels/test_corrigido.tsv', 'run-pesquisa-densa-sentence-transformer-all-mpnet-base-v2.txt']\n",
            "Results:\n",
            "ndcg_cut_10           \tall\t0.5133\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "model = SentenceTransformer('all-MiniLM-L12-v2').to(device)\n",
        "\n",
        "if gerar_arquivo_doc_embeddings_all_MiniLM_L12_v2:\n",
        "  docs_embeddings = model.encode(textos_trec_covid)\n",
        "  torch.save(docs_embeddings, arq_doc_embeddings_all_MiniLM_L12_v2)\n",
        "else:\n",
        "  docs_embeddings = torch.load(arq_doc_embeddings_all_MiniLM_L12_v2)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUpCtFUckYex",
        "outputId": "a923d05a-edf3-4206-e2b9-a123cd852e8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2.38 s, sys: 1.05 s, total: 3.43 s\n",
            "Wall time: 3.4 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "run_all_queries_sentence_transformer(model, docs_embeddings, 'run-pesquisa-densa-sentence-transformer-all-MiniLM-L12-v2.txt')\n",
        "!python -m pyserini.eval.trec_eval -c -m ndcg_cut.10 collections/trec-covid/qrels/test_corrigido.tsv run-pesquisa-densa-sentence-transformer-all-MiniLM-L12-v2.txt #type: ignore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-8ses99k0wd",
        "outputId": "3cb53009-158a-484a-cbe6-f188ef8b75f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregando as queries do arquivo queries.jsonl...\n",
            "\n",
            "Total de queries que serão avaliadas: 50\n",
            "0 queries completadas\n",
            "5 queries completadas\n",
            "10 queries completadas\n",
            "15 queries completadas\n",
            "20 queries completadas\n",
            "25 queries completadas\n",
            "30 queries completadas\n",
            "35 queries completadas\n",
            "40 queries completadas\n",
            "45 queries completadas\n",
            "50 queries completadas\n",
            "Downloading https://search.maven.org/remotecontent?filepath=uk/ac/gla/dcs/terrierteam/jtreceval/0.0.5/jtreceval-0.0.5-jar-with-dependencies.jar to /root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar...\n",
            "/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar already exists!\n",
            "Skipping download.\n",
            "Running command: ['java', '-jar', '/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar', '-c', '-m', 'ndcg_cut.10', 'collections/trec-covid/qrels/test_corrigido.tsv', 'run-pesquisa-densa-sentence-transformer-all-MiniLM-L12-v2.txt']\n",
            "Results:\n",
            "ndcg_cut_10           \tall\t0.5082\n",
            "CPU times: user 7.42 s, sys: 7.18 s, total: 14.6 s\n",
            "Wall time: 13.4 s\n"
          ]
        }
      ]
    }
  ]
}