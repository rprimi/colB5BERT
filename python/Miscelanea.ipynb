{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNHYfqhxD5o6ihFOPPFvAyE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rprimi/colB5BERT/blob/main/python/Miscelanea.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFairBH2RBQb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving embeddings to disk is a common task when working with models like BERT, because it allows you to avoid recomputing the embeddings for the same input multiple times. There are several ways to store embeddings, but two popular methods are:\n",
        "\n",
        "1. Saving as numpy arrays using np.save or np.savez (for multiple arrays at once).\n",
        "2. Saving as a pickle file, which is a Python-specific binary format.\n",
        "\n",
        "In your case, since you have a dictionary where each entry is a list of PyTorch tensors, you can first convert your tensors to numpy arrays, and then use one of these methods to store them. Here is an example of how you can do this with numpy:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "def save_embeddings_to_disk(embeddings, filename):\n",
        "    # Convert tensors to numpy arrays and store them in the same structure\n",
        "    numpy_embeddings = {layer: [t.numpy() for t in tensors] for layer, tensors in embeddings.items()}\n",
        "\n",
        "    # Use numpy's savez function to store the dictionary\n",
        "    # We use ** to unpack the dictionary into keyword arguments\n",
        "    np.savez(filename, **numpy_embeddings)\n",
        "```\n",
        "\n",
        "Then, you can load your embeddings back with:\n",
        "\n",
        "```python\n",
        "def load_embeddings_from_disk(filename):\n",
        "    with np.load(filename) as data:\n",
        "        embeddings = {layer: data[layer] for layer in data.files}\n",
        "    return embeddings\n",
        "```\n",
        "\n",
        "This will give you a dictionary where the keys are the layers, and the values are lists of numpy arrays.\n",
        "\n",
        "Please note that you may have to handle large files depending on the size of your embeddings and the number of examples. This can be managed by saving in chunks or using compressed formats if necessary."
      ],
      "metadata": {
        "id": "HhllYoievKsa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Python equivalent of R str\n",
        "\n",
        "Yes, there is a similar command in Python called `dir()` which returns a list of all the attributes and methods of any object passed to it¹. Another similar command is `vars()` which returns the __dict__ attribute of an object¹. There is also a function called `inspect.getmembers()` which returns all the members of an object in a list of (name, value) pairs sorted by name¹. I hope this helps!\n",
        "\n",
        "Origem: conversa com o Bing, 05/06/2023\n",
        "(1) Is there a Python equivalent of R's str (), returning only the .... https://stackoverflow.com/questions/27749573/is-there-a-python-equivalent-of-rs-str-returning-only-the-structure-of-an-ob.\n",
        "(2) What are Python pandas equivalents for R functions like str(), summary .... https://stackoverflow.com/questions/27637281/what-are-python-pandas-equivalents-for-r-functions-like-str-summary-and-he.\n",
        "(3) Qual é a diferença entre 'string' e r'string' em Python?. https://pt.stackoverflow.com/questions/80545/qual-%c3%a9-a-diferen%c3%a7a-entre-string-e-rstring-em-python.\n",
        "\n",
        "Understanding complex data structures in Python code often requires carefully examining the code and using built-in Python functions that give insights about these structures. Here are a few steps that might help:\n",
        "\n",
        "1. **Print Statements**: Use `print()` statements liberally to output variables and their types. This can give you an idea of what data structures are being used at various points in the code.\n",
        "\n",
        "2. **Type Checking**: Use the `type()` function to check the type of data structures. For instance, `type(my_var)` would return the type of `my_var`.\n",
        "\n",
        "3. **Introspection**: Use dir() to view the attributes and methods of an object. For example, `dir(my_var)` would list all the methods that can be used with `my_var`.\n",
        "\n",
        "4. **Length and Structure**: Use `len()` to find the length of a data structure. For dictionaries, lists, tuples, etc., you can also print individual elements.\n",
        "\n",
        "5. **Variable Explorer**: If you're using an Integrated Development Environment (IDE) like PyCharm or Jupyter notebook, you can make use of the variable explorer to inspect your variables and data structures.\n",
        "\n",
        "6. **Debugger**: A debugger can help you step through the code one line at a time and examine the changes in your data structures as the code executes. Python's built-in debugger is pdb.\n",
        "\n",
        "7. **Visualization Tools**: For complex data structures like nested dictionaries or dataframes, consider using data visualization tools or libraries like pandas, matplotlib, or seaborn to visualize the data.\n",
        "\n",
        "Remember, understanding complex data structures can be challenging, but it is often a matter of breaking down the structure into smaller, more manageable parts and understanding those individually.\n",
        "\n",
        "In Python, lists and dictionaries don't have dimensions in the way that arrays in NumPy or dataframes in pandas do. Instead, they have lengths, and those lengths can be nested. You can use the built-in `len()` function to find out the number of elements in a list or dictionary.\n",
        "\n",
        "For a list:\n",
        "\n",
        "```python\n",
        "my_list = [1, 2, 3, 4, 5]\n",
        "print(len(my_list))  # Output: 5\n",
        "```\n",
        "\n",
        "For a dictionary:\n",
        "\n",
        "```python\n",
        "my_dict = {'one': 1, 'two': 2, 'three': 3}\n",
        "print(len(my_dict))  # Output: 3\n",
        "```\n",
        "\n",
        "For nested structures, you'd need to use additional `len()` calls or use a loop or comprehension to iterate over the elements.\n",
        "\n",
        "For example, for a list of lists (a 2D list), you could use a list comprehension:\n",
        "\n",
        "```python\n",
        "my_list_of_lists = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
        "dimensions = [len(inner_list) for inner_list in my_list_of_lists]\n",
        "print(dimensions)  # Output: [3, 3, 3]\n",
        "```\n",
        "\n",
        "This tells you that you have a \"3x3\" list. Note that this only works for regularly-shaped data; if your lists have differing lengths, you'll get a variety of numbers.\n",
        "\n",
        "For a nested dictionary, things can get more complex, and you may need a recursive function to fully explore the structure if the nesting can be more than one level deep."
      ],
      "metadata": {
        "id": "sc45TpDx-gOz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Old functions"
      ],
      "metadata": {
        "id": "xzPeXX_wYX3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def calculate_cosine_similarity(list_A, list_B):\n",
        "    result = []\n",
        "    for tensor_A in list_A:\n",
        "        for tensor_B in list_B:\n",
        "            similarity = cosine_similarity(tensor_A.unsqueeze(0), tensor_B.unsqueeze(0), dim=-1)\n",
        "            result.append(similarity)\n",
        "    return result\n",
        "\n",
        "\n",
        "def calculate_cosine_similarity(list_A, list_B):\n",
        "    result = []\n",
        "    for tensor_A in list_A:\n",
        "        for tensor_B in list_B:\n",
        "            # Expand dimensions so that shapes are [1, n_tokens_A, 768] and [n_tokens_B, 1, 768]\n",
        "            tensor_A_exp = tensor_A.unsqueeze(0).unsqueeze(1)\n",
        "            tensor_B_exp = tensor_B.unsqueeze(0).unsqueeze(2)\n",
        "\n",
        "            # Repeat tensors so that shapes are [n_tokens_B, n_tokens_A, 768] and [n_tokens_B, n_tokens_A, 768]\n",
        "            tensor_A_rep = tensor_A_exp.repeat(tensor_B.shape[0], 1, 1)\n",
        "            tensor_B_rep = tensor_B_exp.repeat(1, tensor_A.shape[0], 1)\n",
        "            similarity = cosine_similarity(tensor_A_rep, tensor_B_rep, dim=-1)\n",
        "            result.append(similarity)\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "SNazzFPSZnvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chat GPT question\n",
        "\n",
        "I have a list of A of i elements. Each elment is a torch tensor of n_tokens1 X 768 (dim of embedings) . Then I have a list of B of p elements. Each elment is a torch tensor of n_tokens2 X 768 (dim of embedings). I want to claculate the cosine similarity between embedding vectors n_tokens1 with n_tokens2. I want the result to be a list of i elements by p elments of matrices n_tokens1 X n_tokens2 containing the cossine similarities. Please creeate a code in python using efficient vectorized operations"
      ],
      "metadata": {
        "id": "dOlXOw7yG7mo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, to compute cosine similarity between tensors, you can use the `torch.nn.functional.cosine_similarity` function provided by PyTorch.\n",
        "\n",
        "Here's a function that computes cosine similarity between every pair of tokens in `A` and `B`, assuming that `A` and `B` are PyTorch tensors.\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def compute_cosine_similarity(A, B):\n",
        "    batch_size1, num_of_tokens1, embedding_dim1 = A.shape\n",
        "    batch_size2, num_of_tokens2, embedding_dim2 = B.shape\n",
        "\n",
        "    if embedding_dim1 != embedding_dim2:\n",
        "        raise ValueError(\"Embedding dimensions must match!\")\n",
        "\n",
        "    if batch_size1 != batch_size2:\n",
        "        raise ValueError(\"Batch sizes must match!\")\n",
        "\n",
        "    # We will compute cosine similarity for each example in the batch separately\n",
        "    cosine_similarities = []\n",
        "    for i in range(batch_size1):\n",
        "        a = A[i]\n",
        "        b = B[i]\n",
        "\n",
        "        # Compute cosine similarity between all pairs of tokens.\n",
        "        # The resulting matrix will have shape (num_of_tokens1, num_of_tokens2)\n",
        "        similarity_matrix = torch.zeros((num_of_tokens1, num_of_tokens2))\n",
        "\n",
        "        for j in range(num_of_tokens1):\n",
        "            for k in range(num_of_tokens2):\n",
        "                similarity_matrix[j, k] = F.cosine_similarity(a[j], b[k], dim=0)\n",
        "\n",
        "        cosine_similarities.append(similarity_matrix)\n",
        "\n",
        "    return cosine_similarities\n",
        "```\n",
        "\n",
        "This function will return a list of 2D tensors, each with shape `(num_of_tokens1, num_of_tokens2)`. Each tensor in the list corresponds to an example in the batch. The values in the 2D tensor represent the cosine similarity between the corresponding tokens in `A` and `B`. Note that this is quite a computationally expensive way to compute these similarities due to the nested for loops, and it might be worth looking into more efficient methods if this becomes a bottleneck in your code.\n",
        "\n",
        "Can you do this using vector operations ?\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def compute_cosine_similarity_vectorized(A, B):\n",
        "    # Check for matching dimensions\n",
        "    batch_size1, num_of_tokens1, embedding_dim1 = A.shape\n",
        "    batch_size2, num_of_tokens2, embedding_dim2 = B.shape\n",
        "\n",
        "    if embedding_dim1 != embedding_dim2:\n",
        "        raise ValueError(\"Embedding dimensions must match!\")\n",
        "\n",
        "    if batch_size1 != batch_size2:\n",
        "        raise ValueError(\"Batch sizes must match!\")\n",
        "\n",
        "    A_unsqueezed = A.unsqueeze(2)  # Shape becomes [batch_size, num_of_tokens1, 1, embedding_dim]\n",
        "    B_unsqueezed = B.unsqueeze(1)  # Shape becomes [batch_size, 1, num_of_tokens2, embedding_dim]\n",
        "\n",
        "    # Calculate cosine similarity. The result has shape [batch_size, num_of_tokens1, num_of_tokens2]\n",
        "    similarity_matrix = F.cosine_similarity(A_unsqueezed, B_unsqueezed, dim=-1)\n",
        "\n",
        "    return similarity_matrix\n",
        "\n",
        "def get_bert_embeddings(bert_model, examples, layers):\n",
        "    # Move model to GPU if available\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    bert_model = bert_model.to(device)\n",
        "\n",
        "    embeddings = {layer: [] for layer in layers}\n",
        "    for ex_ids in examples:\n",
        "        # Convert data to tensor and move to GPU\n",
        "        ex_ids_tensor = torch.tensor([ex_ids]).to(device)\n",
        "        with torch.no_grad():\n",
        "            # Output includes 'last_hidden_state', 'pooler_output', 'hidden_states'\n",
        "            output = bert_model(ex_ids_tensor, output_hidden_states=True)\n",
        "            hidden_states = output.hidden_states\n",
        "            for layer in layers:\n",
        "                # Verify layer index is valid\n",
        "                if layer < 0 or layer >= len(hidden_states):\n",
        "                    print(f\"Invalid layer {layer}\")\n",
        "                else:\n",
        "                    # Hidden states is a tuple. Indexing into it gives a tensor of shape\n",
        "                    # (batch_size, sequence_length, hidden_size). Since batch_size is 1,\n",
        "                    # we remove the batch dimension.\n",
        "                    layer_output = hidden_states[layer].squeeze(0)\n",
        "                    # Convert back to CPU for further processing or storage\n",
        "                    embeddings[layer].append(layer_output.to('cpu'))\n",
        "    return embeddings\n",
        "```\n"
      ],
      "metadata": {
        "id": "r0cLFImnZnGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def calculate_cosine_similarity(list_A, list_B, topk=5):\n",
        "    result = []\n",
        "    for tensor_A in list_A:\n",
        "        similarities = []\n",
        "        for tensor_B in list_B:\n",
        "            # Expand dimensions so that shapes are [n_tokens_A, 1, 768] and [1, n_tokens_B, 768]\n",
        "            tensor_A_exp = tensor_A.unsqueeze(1)\n",
        "            tensor_B_exp = tensor_B.unsqueeze(0)\n",
        "            # Compute cosine similarity\n",
        "            similarity = torch.nn.functional.cosine_similarity(tensor_A_exp, tensor_B_exp, dim=-1)\n",
        "            similarities.append(similarity)\n",
        "        # Concatenate all similarity scores and find the topk\n",
        "        similarities = torch.cat(similarities, dim=-1)\n",
        "        topk_values, topk_indices = torch.topk(similarities, topk, dim=-1)\n",
        "        result.append((topk_values, topk_indices))\n",
        "    return result\n",
        "\n",
        "\n",
        "def calculate_cosine_similarity(list_A, list_B, topk=5, batch_size=10):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    result = []\n",
        "\n",
        "    # Move tensors to GPU\n",
        "    list_A = [tensor.to(device) for tensor in list_A]\n",
        "    list_B = [tensor.to(device) for tensor in list_B]\n",
        "\n",
        "    # Process batch_size tensor_A at a time\n",
        "    for i in range(0, len(list_A), batch_size):\n",
        "        batch_A = list_A[i:i+batch_size]\n",
        "        batch_A = torch.stack(batch_A).unsqueeze(2)  # [batch_size, n_tokens_A, 1, 768]\n",
        "        similarities = []\n",
        "\n",
        "        for tensor_B in list_B:\n",
        "            tensor_B = tensor_B.unsqueeze(0).unsqueeze(0)  # [1, 1, n_tokens_B, 768]\n",
        "            similarity = torch.nn.functional.cosine_similarity(batch_A, tensor_B, dim=-1)  # [batch_size, n_tokens_A, n_tokens_B]\n",
        "            similarities.append(similarity)\n",
        "\n",
        "        similarities = torch.cat(similarities, dim=-1)  # [batch_size, n_tokens_A, n_tokens_B*len(list_B)]\n",
        "        topk_values, topk_indices = torch.topk(similarities, topk, dim=-1)  # [batch_size, n_tokens_A, topk]\n",
        "        result.extend(zip(topk_values, topk_indices))\n",
        "\n",
        "    # Move tensors back to CPU\n",
        "    result = [(values.to('cpu'), indices.to('cpu')) for values, indices in result]\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def calculate_cosine_similarity(list_A, list_B, topk=5, batch_size=10):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    result = []\n",
        "\n",
        "    # Move tensors to GPU\n",
        "    list_A = [tensor.to(device) for tensor in list_A]\n",
        "    list_B = [tensor.to(device) for tensor in list_B]\n",
        "\n",
        "    # Process batch_size tensor_A at a time\n",
        "    for i in tqdm(range(0, len(list_A), batch_size), desc='Processing', dynamic_ncols=True):\n",
        "        batch_A = list_A[i:i+batch_size]\n",
        "        batch_A = torch.nn.utils.rnn.pad_sequence(batch_A, batch_first=True).unsqueeze(2)  # [batch_size, max_n_tokens_A, 1, 768]\n",
        "        similarities = []\n",
        "\n",
        "        for tensor_B in list_B:\n",
        "            tensor_B = tensor_B.unsqueeze(0).unsqueeze(0)  # [1, 1, n_tokens_B, 768]\n",
        "            similarity = torch.nn.functional.cosine_similarity(batch_A, tensor_B, dim=-1)  # [batch_size, max_n_tokens_A, n_tokens_B]\n",
        "            similarities.append(similarity)\n",
        "\n",
        "        similarities = torch.cat(similarities, dim=-1)  # [batch_size, max_n_tokens_A, n_tokens_B*len(list_B)]\n",
        "        topk_values, topk_indices = torch.topk(similarities, topk, dim=-1)  # [batch_size, max_n_tokens_A, topk]\n",
        "        result.extend(zip(topk_values, topk_indices))\n",
        "\n",
        "    # Move tensors back to CPU\n",
        "    result = [(values.to('cpu'), indices.to('cpu')) for values, indices in result]\n",
        "\n",
        "    return result\n",
        "\n"
      ],
      "metadata": {
        "id": "B7c_PajzI_-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm /content/drive/MyDrive/*.npz\n"
      ],
      "metadata": {
        "id": "58I16puj-8Dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mysql-connector-python\n",
        "import mysql.connector\n",
        "\n",
        "\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)  # you can change the level to DEBUG if you want more detailed logs\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def calculate_cosine_similarity1_db(list_A, list_B, db_config, table_name, batch_size=1):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Move list_A tensors to GPU\n",
        "    list_A = [tensor.to(device) for tensor in list_A]\n",
        "\n",
        "    # Connect to the MySQL server\n",
        "    cnx = mysql.connector.connect(**db_config)\n",
        "    cursor = cnx.cursor()\n",
        "\n",
        "    # Create a new table\n",
        "    cursor.execute(f\"CREATE TABLE IF NOT EXISTS {table_name} (n_batch INT, num_element_listA INT, num_token_A INT, num_element_list_B INT, num_token_B INT, cos_sim FLOAT)\")\n",
        "\n",
        "    # Process batch_size tensor_A at a time\n",
        "    for n_batch in tqdm(range(0, len(list_A), batch_size), desc='Processing', dynamic_ncols=True):\n",
        "        batch_A = list_A[n_batch:n_batch+batch_size]\n",
        "        batch_A = torch.nn.utils.rnn.pad_sequence(batch_A, batch_first=True).unsqueeze(2)  # [batch_size, max_n_tokens_A, 1, 768]\n",
        "        batch_data = []\n",
        "\n",
        "        for num_element_list_B, tensor_B in enumerate(list_B):\n",
        "            tensor_B = tensor_B.to(device).unsqueeze(0).unsqueeze(0)  # [1, 1, n_tokens_B, 768]\n",
        "            similarity = torch.nn.functional.cosine_similarity(batch_A, tensor_B, dim=-1)  # [batch_size, max_n_tokens_A, n_tokens_B]\n",
        "            similarity = similarity.half()  # Reduce precision to float16\n",
        "            similarity_np = similarity.cpu().numpy()  # Move similarity to CPU and convert to NumPy array\n",
        "\n",
        "            # Iterate over the elements in the similarity matrix\n",
        "            for num_element_listA in range(similarity_np.shape[0]):\n",
        "                for num_token_A in range(similarity_np.shape[1]):\n",
        "                    for num_token_B in range(similarity_np.shape[2]):\n",
        "                        # Extract the corresponding cosine similarity\n",
        "                        cos_sim = similarity_np[num_element_listA, num_token_A, num_token_B]\n",
        "\n",
        "                        # Add the row to the batch_data list\n",
        "                        batch_data.append((n_batch, num_element_listA, num_token_A, num_element_list_B, num_token_B, float(cos_sim)))\n",
        "\n",
        "            del tensor_B  # Delete tensor_B from GPU memory\n",
        "\n",
        "        # Insert all rows in batch_data into the database\n",
        "        query = f\"INSERT INTO {table_name} (n_batch, num_element_listA, num_token_A, num_element_list_B, num_token_B, cos_sim) VALUES (%s, %s, %s, %s, %s, %s)\"\n",
        "        cursor.executemany(query, batch_data)\n",
        "\n",
        "        logger.info(f\"Inserted {len(batch_data)} rows into the database\")\n",
        "\n",
        "    # Make sure data is committed to the database\n",
        "    cnx.commit()\n",
        "    cursor.close()\n",
        "    cnx.close()\n",
        "\n",
        "# Example usage\n",
        "db_config = {'host': 'localhost', 'database': 'test_db', 'user': 'test_user', 'password': 'test_password'}\n",
        "calculate_cosine_similarity1_db(list_A, list_B, db_config=db_config, table\n",
        "\n"
      ],
      "metadata": {
        "id": "du4qj1DApf0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# print(batch)\n",
        "# print(element_A)\n",
        "# print(batch_A.shape[1])\n",
        "# print(element_B)\n",
        "# print(tensor_B.shape[2])\n",
        "# print(similarity[element_A])\n",
        "\n",
        "calculate_cosine_similarity1_db(embeddings_itens[6], embeddings_posts[6], db_config=db_config,table_name=\"cos_sim_L6\",  batch_size=1)\n"
      ],
      "metadata": {
        "id": "eUsAXqmSrWBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(level=logging.INFO)  # you can change the level to DEBUG if you want more detailed logs\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def calculate_cosine_similarity1_hdf5b(list_A, list_B, filename, dataset_name, max_tokens_A, max_tokens_B, num_batches, num_list_A, num_list_B, batch_size=1):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Move list_A tensors to GPU\n",
        "    list_A = [tensor.to(device) for tensor in list_A]\n",
        "\n",
        "    # Create a new HDF5 file\n",
        "    with h5py.File(filename, \"w\") as f:\n",
        "        # Create the dataset\n",
        "        dset = f.create_dataset(dataset_name, (num_batches, num_list_A, num_list_B, max_tokens_A, max_tokens_B), dtype='float16')\n",
        "\n",
        "        # Process batch_size tensor_A at a time\n",
        "        for n_batch in tqdm(range(0, len(list_A), batch_size), desc='Processing', dynamic_ncols=True):\n",
        "            batch_A = list_A[n_batch:n_batch+batch_size]\n",
        "            batch_A = torch.nn.utils.rnn.pad_sequence(batch_A, batch_first=True).unsqueeze(2)  # [batch_size, max_n_tokens_A, 1, 768]\n",
        "\n",
        "            for n_listA_elem, tensor_A in enumerate(batch_A):\n",
        "                for n_listB_elem, tensor_B in enumerate(list_B):\n",
        "                    tensor_B = tensor_B.to(device).unsqueeze(0).unsqueeze(0)  # [1, 1, n_tokens_B, 768]\n",
        "                    similarity = torch.nn.functional.cosine_similarity(tensor_A.unsqueeze(0), tensor_B, dim=-1)  # [1, n_tokens_A, n_tokens_B]\n",
        "                    similarity = similarity.half()  # Reduce precision to float16\n",
        "                    similarity_np = similarity.cpu().numpy()  # Move similarity to CPU and convert to NumPy array\n",
        "\n",
        "                    # Write the similarities to the HDF5 dataset\n",
        "                    dset[n_batch, n_listA_elem, n_listB_elem, :similarity_np.shape[1], :similarity_np.shape[2]] = similarity_np\n",
        "\n",
        "                    del tensor_B  # Delete tensor_B from GPU memory\n",
        "                    logger.info(f\"batch: {n_batch}, A elem: {n_listA_elem}, B elem: {n_listB_elem} , similarity shape: {similarity_np.shape[1]} X {similarity_np.shape[2]}\")\n"
      ],
      "metadata": {
        "id": "Ms010bDDVFai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cosim_top5_L6 = calculate_cosine_similarity2(embeddings_itens[6][0:3], embeddings_posts[6][0:10], topk = 5, batch_size=2)\n",
        "filname=\"cosim_L11\"\n",
        "dataset_name = \"db_cosim_L1\"\n",
        "max_tokens_A = 512\n",
        "max_tokens_B = 23\n",
        "num_batches = len(embeddings_posts[11])\n",
        "num_list_A = len(embeddings_posts[11])\n",
        "num_list_B = len(embeddings_itens[11])\n",
        "batch_size=1\n",
        "\n",
        "calculate_cosine_similarity1_hdf5b(embeddings_posts[11], embeddings_itens[11], filname, dataset_name, max_tokens_A, max_tokens_B, num_batches, num_list_A, num_list_B, batch_size)"
      ],
      "metadata": {
        "id": "H9fzJ176bRBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "def calculate_cosine_similarity1_hdf5(list_A, list_B, filename, dataset_name, batch_size=1):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Move list_A tensors to GPU\n",
        "    list_A = [tensor.to(device) for tensor in list_A]\n",
        "\n",
        "    # Create a new HDF5 file\n",
        "    with h5py.File(filename, \"w\") as f:\n",
        "        # Process batch_size tensor_A at a time\n",
        "        for n_batch in tqdm(range(0, len(list_A), batch_size), desc='Processing', dynamic_ncols=True):\n",
        "            batch_A = list_A[n_batch:n_batch+batch_size]\n",
        "            batch_A = torch.nn.utils.rnn.pad_sequence(batch_A, batch_first=True).unsqueeze(2)  # [batch_size, max_n_tokens_A, 1, 768]\n",
        "\n",
        "            for num_element_list_B, tensor_B in enumerate(list_B):\n",
        "                tensor_B = tensor_B.to(device).unsqueeze(0).unsqueeze(0)  # [1, 1, n_tokens_B, 768]\n",
        "                similarity = torch.nn.functional.cosine_similarity(batch_A, tensor_B, dim=-1)  # [batch_size, max_n_tokens_A, n_tokens_B]\n",
        "                similarity = similarity.half()  # Reduce precision to float16\n",
        "                similarity_np = similarity.cpu().numpy()  # Move similarity to CPU and convert to NumPy array\n",
        "\n",
        "                # Create a new dataset for each batch, element of list_A, and element of list_B\n",
        "                dset = f.create_dataset(f\"{dataset_name}/batch_{n_batch}_A_{num_element_list_B}_B\", data=similarity_np, dtype='float16')\n",
        "\n",
        "                del tensor_B  # Delete tensor_B from GPU memory\n"
      ],
      "metadata": {
        "id": "OavwvwsKXLEK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}