{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rprimi/colB5BERT/blob/main/python/b5_contextualreps_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8TP1m0F_Y6-H"
      },
      "outputs": [],
      "source": [
        "__author__ = \"Ricardo Primi adapted from modules from Christopher Potts, CS224u, Stanford, Spring 2021\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnEbhqVTY6-J"
      },
      "source": [
        "### General set-up\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZUio6VJkQfQ"
      },
      "outputs": [],
      "source": [
        "!pip3 install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/rprimi/colB5BERT.git\n",
        "\n",
        "%cd /content/colB5BERT\n",
        "!git pull\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8evBd6sMsqT",
        "outputId": "b6728e2c-041a-4543-eb18-4aa7ffaa8260"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/colB5BERT\n",
            "remote: Enumerating objects: 11, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 6 (delta 5), reused 6 (delta 5), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (6/6), 175.43 KiB | 387.00 KiB/s, done.\n",
            "From https://github.com/rprimi/colB5BERT\n",
            "   b50f673..4bd1c18  main       -> origin/main\n",
            "Updating b50f673..4bd1c18\n",
            "Fast-forward\n",
            " R/dados_python.qmd          |     2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " data/db_textos.splitted.csv | 25711 \u001b[32m+++++++++++++++++++\u001b[m\u001b[31m-----------------------\u001b[m\n",
            " 2 files changed, 11480 insertions(+), 14233 deletions(-)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FneXYJMlY6-K"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertModel, BertTokenizer\n",
        "from transformers import RobertaModel, RobertaTokenizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOgZt0MKWbOI"
      },
      "source": [
        "Modules `vsm`, `utils` and `sst` are from Stanford's CS224u https://github.com/cgpotts/cs224u"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wIOgkGQmkk-5"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/colB5BERT/python/')\n",
        "\n",
        "import utils\n",
        "import vsm\n",
        "import sst\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DSpFAHTsFF-",
        "outputId": "c2057e13-ec42-4719-e3b7-d1d6e8bf5f6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda:0\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available(): \n",
        "   dev = \"cuda:0\"\n",
        "else: \n",
        "   dev = \"cpu\"\n",
        "device = torch.device(dev)\n",
        "print('Using {}'.format(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0iX5kbasHZd"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dTrPrG2kdCz"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pboznS998H17",
        "outputId": "bbdf68c1-f59b-4da0-cb43-25ea8b98e90c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 11537 entries, 0 to 11536\n",
            "Data columns (total 3 columns):\n",
            " #   Column          Non-Null Count  Dtype \n",
            "---  ------          --------------  ----- \n",
            " 0   id              11537 non-null  int64 \n",
            " 1   id_divisao      11537 non-null  int64 \n",
            " 2   texto_dividido  11537 non-null  object\n",
            "dtypes: int64(2), object(1)\n",
            "memory usage: 270.5+ KB\n"
          ]
        }
      ],
      "source": [
        "b5_data = pd.read_csv('/content/colB5BERT/data/db_textos.splitted.csv', sep=';') \n",
        "b5_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b5_data "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "ZX_oY24kO-Tg",
        "outputId": "8d96601e-1068-440f-f391-3a7a3a182317"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id  id_divisao                                     texto_dividido\n",
              "0      100           1  ajudando porque a zuzu é um amor e tem a voz f...\n",
              "1      100           2  vai ter share sim e se reclamar dou share mais...\n",
              "2      100           3  quanto parece , A , $NUMBER$ . MvC $NUMBER$ CL...\n",
              "3      100           4  $NUMBER$ jeitos de dar entry então é só sucess...\n",
              "4      100           5  quiser se vira Esse livro é de co-autoria de $...\n",
              "...    ...         ...                                                ...\n",
              "11532  999           6  amo muito ! < $NUMBER$ < $NUMBER$ \" Fique por ...\n",
              "11533  999           7  pai ! Feliz aniversário ! < $NUMBER$ < $NUMBER...\n",
              "11534  999           8  rei do $NAME$ Club de $NAME$ Oeste : $NAME$ $N...\n",
              "11535  999           9  todo tipo de público . A realização do projeto...\n",
              "11536  999          10  sobre sua vida , e as mais recentes ajudam a c...\n",
              "\n",
              "[11537 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a80e4e54-635c-4510-a229-f8298de88509\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>id_divisao</th>\n",
              "      <th>texto_dividido</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100</td>\n",
              "      <td>1</td>\n",
              "      <td>ajudando porque a zuzu é um amor e tem a voz f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>100</td>\n",
              "      <td>2</td>\n",
              "      <td>vai ter share sim e se reclamar dou share mais...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100</td>\n",
              "      <td>3</td>\n",
              "      <td>quanto parece , A , $NUMBER$ . MvC $NUMBER$ CL...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>100</td>\n",
              "      <td>4</td>\n",
              "      <td>$NUMBER$ jeitos de dar entry então é só sucess...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>100</td>\n",
              "      <td>5</td>\n",
              "      <td>quiser se vira Esse livro é de co-autoria de $...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11532</th>\n",
              "      <td>999</td>\n",
              "      <td>6</td>\n",
              "      <td>amo muito ! &lt; $NUMBER$ &lt; $NUMBER$ \" Fique por ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11533</th>\n",
              "      <td>999</td>\n",
              "      <td>7</td>\n",
              "      <td>pai ! Feliz aniversário ! &lt; $NUMBER$ &lt; $NUMBER...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11534</th>\n",
              "      <td>999</td>\n",
              "      <td>8</td>\n",
              "      <td>rei do $NAME$ Club de $NAME$ Oeste : $NAME$ $N...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11535</th>\n",
              "      <td>999</td>\n",
              "      <td>9</td>\n",
              "      <td>todo tipo de público . A realização do projeto...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11536</th>\n",
              "      <td>999</td>\n",
              "      <td>10</td>\n",
              "      <td>sobre sua vida , e as mais recentes ajudam a c...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11537 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a80e4e54-635c-4510-a229-f8298de88509')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a80e4e54-635c-4510-a229-f8298de88509 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a80e4e54-635c-4510-a229-f8298de88509');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JS59UBqrY6-L"
      },
      "source": [
        "### Loading Transformer models\n",
        "Specify a model, a tokenizer, and load a model pretrained weights:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6f6UXqGY6-L"
      },
      "outputs": [],
      "source": [
        "bert_weights_name = 'neuralmind/bert-base-portuguese-cased'\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(bert_weights_name)\n",
        "bert_model = BertModel.from_pretrained(bert_weights_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHrst3TZY6-M"
      },
      "source": [
        "### The basics of tokenizing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOg05YkzY6-M"
      },
      "outputs": [],
      "source": [
        "import textwrap\n",
        "\n",
        "def print_cell(df, row, column, wrap_length=80):\n",
        "    if row < len(df) and column in df.columns:\n",
        "        text = df.loc[row, column]\n",
        "        print('\\n'.join(textwrap.wrap(text, width=wrap_length)))\n",
        "    else:\n",
        "        print(\"Invalid row or column\")\n",
        "\n",
        "print_cell(b5_data, 3, 'texto_dividido')\n",
        "\n",
        "ex_ids = bert_tokenizer.encode(b5_data.loc[3, 'texto_dividido'], add_special_tokens=True)\n",
        "bert_tokenizer.convert_ids_to_tokens(ex_ids)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDRVTaxRY6-P"
      },
      "source": [
        "### Getting BERT embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wV8TNuFDY6-P"
      },
      "source": [
        "To obtain the representations for a batch of examples, we use the `forward` method of the model, as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9BLypVjY6-P"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    reps = bert_model(torch.tensor([ex_ids]), output_hidden_states=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_texts(texts):\n",
        "    # Tokenize each text and convert to input IDs\n",
        "    input_ids = [bert_tokenizer.encode(text, add_special_tokens=True) for text in texts]\n",
        "    return input_ids\n",
        "\n",
        "\n",
        "def tokenize_texts(bert_tokenizer, texts):\n",
        "    tokenized_texts = []\n",
        "    for text in texts:\n",
        "        encoded_text = bert_tokenizer.encode(text, add_special_tokens=True)\n",
        "        # truncate the encoded text to the first 512 tokens\n",
        "        encoded_text = encoded_text[:512]\n",
        "        # encoded_text = encoded_text\n",
        "        tokenized_texts.append(encoded_text)\n",
        "    return tokenized_texts\n",
        "\n"
      ],
      "metadata": {
        "id": "r11vxToiVrfP"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize_texts(ex_of_texts)\n",
        "ex_of_texts = b5_data.iloc[[0, 1, 2, 3], b5_data.columns.get_loc('texto_dividido')].tolist()\n",
        "lengths = [len(sublist) for sublist in tokenize_texts(bert_tokenizer, ex_of_texts)]\n",
        "\n",
        "print(lengths)  # Output: [3, 2, 4]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2ddFICvXD7F",
        "outputId": "ccbcb8cb-dc4f-45fd-c6bc-481a6d798bb2"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[512, 512, 477, 512]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def get_bert_embeddings(bert_model, examples, layers):\n",
        "    embeddings = {layer: [] for layer in layers}\n",
        "    for ex_ids in examples:\n",
        "        with torch.no_grad():\n",
        "            # Output includes 'last_hidden_state', 'pooler_output', 'hidden_states'\n",
        "            output = bert_model(torch.tensor([ex_ids]), output_hidden_states=True)\n",
        "            hidden_states = output.hidden_states\n",
        "            for layer in layers:\n",
        "                # Verify layer index is valid\n",
        "                if layer < 0 or layer >= len(hidden_states):\n",
        "                    print(f\"Invalid layer {layer}\")\n",
        "                else:\n",
        "                    # Hidden states is a tuple. Indexing into it gives a tensor of shape \n",
        "                    # (batch_size, sequence_length, hidden_size). Since batch_size is 1,\n",
        "                    # we remove the batch dimension.\n",
        "                    layer_output = hidden_states[layer].squeeze(0)\n",
        "                    embeddings[layer].append(layer_output)\n",
        "    return embeddings\n"
      ],
      "metadata": {
        "id": "6uybcKQ_VFQO"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layers = [6, 9, 11, 12]  # Specify the layers you want\n",
        "embeddings = get_bert_embeddings(bert_model, tokenize_texts(bert_tokenizer, ex_of_texts), layers)\n",
        "\n",
        "import pprint\n",
        "pprint.pprint(embeddings )\n",
        "\n",
        "\n",
        "dir(embeddings)\n",
        "vars(embeddings)\n",
        "import inspect\n",
        "inspect.getmembers(embeddings)\n",
        "\n",
        "pprint.pprint(embeddings[9])\n",
        "len(embeddings[9])\n",
        "len(embeddings[9][2])\n",
        "pprint.pprint(embeddings[9][2])\n",
        "x = embeddings[9][2]\n",
        "x.shape\n",
        "\n",
        "dimensions = [len(inner_list) for inner_list in embeddings[9][0]]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpAtS9ZIVH8j",
        "outputId": "aa9383f4-8e8a-4eae-815a-c8deab1863a3"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([477, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embeddings is a dict of `layers` keys. Each component of one key is composed of `batch size` elments of a tensor with size `num_of_tokens X embedding_dim`. \n",
        "\n",
        "The final structure is `layers X batch size X num_of_tokens X embedding_dim`"
      ],
      "metadata": {
        "id": "t4aDJx91AehR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Python equivalent of R str\n",
        "\n",
        "Yes, there is a similar command in Python called `dir()` which returns a list of all the attributes and methods of any object passed to it¹. Another similar command is `vars()` which returns the __dict__ attribute of an object¹. There is also a function called `inspect.getmembers()` which returns all the members of an object in a list of (name, value) pairs sorted by name¹. I hope this helps!\n",
        "\n",
        "Origem: conversa com o Bing, 05/06/2023\n",
        "(1) Is there a Python equivalent of R's str (), returning only the .... https://stackoverflow.com/questions/27749573/is-there-a-python-equivalent-of-rs-str-returning-only-the-structure-of-an-ob.\n",
        "(2) What are Python pandas equivalents for R functions like str(), summary .... https://stackoverflow.com/questions/27637281/what-are-python-pandas-equivalents-for-r-functions-like-str-summary-and-he.\n",
        "(3) Qual é a diferença entre 'string' e r'string' em Python?. https://pt.stackoverflow.com/questions/80545/qual-%c3%a9-a-diferen%c3%a7a-entre-string-e-rstring-em-python.\n",
        "\n",
        "Understanding complex data structures in Python code often requires carefully examining the code and using built-in Python functions that give insights about these structures. Here are a few steps that might help:\n",
        "\n",
        "1. **Print Statements**: Use `print()` statements liberally to output variables and their types. This can give you an idea of what data structures are being used at various points in the code.\n",
        "\n",
        "2. **Type Checking**: Use the `type()` function to check the type of data structures. For instance, `type(my_var)` would return the type of `my_var`.\n",
        "\n",
        "3. **Introspection**: Use dir() to view the attributes and methods of an object. For example, `dir(my_var)` would list all the methods that can be used with `my_var`.\n",
        "\n",
        "4. **Length and Structure**: Use `len()` to find the length of a data structure. For dictionaries, lists, tuples, etc., you can also print individual elements.\n",
        "\n",
        "5. **Variable Explorer**: If you're using an Integrated Development Environment (IDE) like PyCharm or Jupyter notebook, you can make use of the variable explorer to inspect your variables and data structures.\n",
        "\n",
        "6. **Debugger**: A debugger can help you step through the code one line at a time and examine the changes in your data structures as the code executes. Python's built-in debugger is pdb.\n",
        "\n",
        "7. **Visualization Tools**: For complex data structures like nested dictionaries or dataframes, consider using data visualization tools or libraries like pandas, matplotlib, or seaborn to visualize the data.\n",
        "\n",
        "Remember, understanding complex data structures can be challenging, but it is often a matter of breaking down the structure into smaller, more manageable parts and understanding those individually.\n",
        "\n",
        "In Python, lists and dictionaries don't have dimensions in the way that arrays in NumPy or dataframes in pandas do. Instead, they have lengths, and those lengths can be nested. You can use the built-in `len()` function to find out the number of elements in a list or dictionary. \n",
        "\n",
        "For a list:\n",
        "\n",
        "```python\n",
        "my_list = [1, 2, 3, 4, 5]\n",
        "print(len(my_list))  # Output: 5\n",
        "```\n",
        "\n",
        "For a dictionary:\n",
        "\n",
        "```python\n",
        "my_dict = {'one': 1, 'two': 2, 'three': 3}\n",
        "print(len(my_dict))  # Output: 3\n",
        "```\n",
        "\n",
        "For nested structures, you'd need to use additional `len()` calls or use a loop or comprehension to iterate over the elements.\n",
        "\n",
        "For example, for a list of lists (a 2D list), you could use a list comprehension:\n",
        "\n",
        "```python\n",
        "my_list_of_lists = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
        "dimensions = [len(inner_list) for inner_list in my_list_of_lists]\n",
        "print(dimensions)  # Output: [3, 3, 3]\n",
        "```\n",
        "\n",
        "This tells you that you have a \"3x3\" list. Note that this only works for regularly-shaped data; if your lists have differing lengths, you'll get a variety of numbers.\n",
        "\n",
        "For a nested dictionary, things can get more complex, and you may need a recursive function to fully explore the structure if the nesting can be more than one level deep."
      ],
      "metadata": {
        "id": "sc45TpDx-gOz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finaly getting the embeddings"
      ],
      "metadata": {
        "id": "k4pHrJ-3Du2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the layers you want\n",
        "layers = [6, 9, 11, 12]  \n",
        "len(b5_data['texto_dividido'].tolist())\n",
        "\n",
        "\n",
        "embeddings = get_bert_embeddings(bert_model, tokenize_texts(bert_tokenizer, b5_data['texto_dividido'].tolist()), layers)"
      ],
      "metadata": {
        "id": "l50Z1laf-fof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwNE2XOqGhuV"
      },
      "source": [
        "### Miscelaneous"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Mnh-cKYY6-X"
      },
      "source": [
        "## Some related work\n",
        "\n",
        "1. [Ethayarajh (2019)](https://www.aclweb.org/anthology/D19-1006/) uses dimensionality reduction techniques (akin to LSA) to derive static representations from contextual models, and explores layer-wise variation in detailed, with findings that are likely to align with your experiences using the above techniques.\n",
        "\n",
        "1. [Akbik et al (2019)](https://www.aclweb.org/anthology/N19-1078/) explore techniques similar to those of Bommasani et al. specifically for the supervised task of named entity recognition.\n",
        "\n",
        "1. [Wang et al. (2020](https://arxiv.org/pdf/1911.02929.pdf) learn static representations from contextual ones using techniques adapted from the word2vec model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, to compute cosine similarity between tensors, you can use the `torch.nn.functional.cosine_similarity` function provided by PyTorch.\n",
        "\n",
        "Here's a function that computes cosine similarity between every pair of tokens in `A` and `B`, assuming that `A` and `B` are PyTorch tensors.\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def compute_cosine_similarity(A, B):\n",
        "    batch_size1, num_of_tokens1, embedding_dim1 = A.shape\n",
        "    batch_size2, num_of_tokens2, embedding_dim2 = B.shape\n",
        "    \n",
        "    if embedding_dim1 != embedding_dim2:\n",
        "        raise ValueError(\"Embedding dimensions must match!\")\n",
        "\n",
        "    if batch_size1 != batch_size2:\n",
        "        raise ValueError(\"Batch sizes must match!\")\n",
        "\n",
        "    # We will compute cosine similarity for each example in the batch separately\n",
        "    cosine_similarities = []\n",
        "    for i in range(batch_size1):\n",
        "        a = A[i]\n",
        "        b = B[i]\n",
        "\n",
        "        # Compute cosine similarity between all pairs of tokens.\n",
        "        # The resulting matrix will have shape (num_of_tokens1, num_of_tokens2)\n",
        "        similarity_matrix = torch.zeros((num_of_tokens1, num_of_tokens2))\n",
        "\n",
        "        for j in range(num_of_tokens1):\n",
        "            for k in range(num_of_tokens2):\n",
        "                similarity_matrix[j, k] = F.cosine_similarity(a[j], b[k], dim=0)\n",
        "\n",
        "        cosine_similarities.append(similarity_matrix)\n",
        "\n",
        "    return cosine_similarities\n",
        "```\n",
        "\n",
        "This function will return a list of 2D tensors, each with shape `(num_of_tokens1, num_of_tokens2)`. Each tensor in the list corresponds to an example in the batch. The values in the 2D tensor represent the cosine similarity between the corresponding tokens in `A` and `B`. Note that this is quite a computationally expensive way to compute these similarities due to the nested for loops, and it might be worth looking into more efficient methods if this becomes a bottleneck in your code.\n",
        "\n",
        "Can you do this using vector operations ?\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def compute_cosine_similarity_vectorized(A, B):\n",
        "    # Check for matching dimensions\n",
        "    batch_size1, num_of_tokens1, embedding_dim1 = A.shape\n",
        "    batch_size2, num_of_tokens2, embedding_dim2 = B.shape\n",
        "\n",
        "    if embedding_dim1 != embedding_dim2:\n",
        "        raise ValueError(\"Embedding dimensions must match!\")\n",
        "\n",
        "    if batch_size1 != batch_size2:\n",
        "        raise ValueError(\"Batch sizes must match!\")\n",
        "\n",
        "    A_unsqueezed = A.unsqueeze(2)  # Shape becomes [batch_size, num_of_tokens1, 1, embedding_dim]\n",
        "    B_unsqueezed = B.unsqueeze(1)  # Shape becomes [batch_size, 1, num_of_tokens2, embedding_dim]\n",
        "\n",
        "    # Calculate cosine similarity. The result has shape [batch_size, num_of_tokens1, num_of_tokens2]\n",
        "    similarity_matrix = F.cosine_similarity(A_unsqueezed, B_unsqueezed, dim=-1)\n",
        "\n",
        "    return similarity_matrix\n"
      ],
      "metadata": {
        "id": "r0cLFImnZnGW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SNazzFPSZnvM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "qwNE2XOqGhuV"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.1 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}