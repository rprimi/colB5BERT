{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rprimi/colB5BERT/blob/main/python/b5_contextualreps_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting BERT embeddings and calculating consine similarity between items and posts tokens\n",
        "Ricardo Primi\n",
        "Projeto Final, UNICAMP, Disciplina IA368 Deep Learning aplicada a buscas"
      ],
      "metadata": {
        "id": "6Gt7xmuV1a1j"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnEbhqVTY6-J"
      },
      "source": [
        "### General set-up\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Rw8Xy2SLvf9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "z1wco2iUqepZ",
        "outputId": "62006a8f-1be8-43e7-d356-6b16255e80ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mZUio6VJkQfQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54f46dda-72ba-4a88-a4f9-0b57ded0c9f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/rprimi/colB5BERT.git\n",
        "\n",
        "%cd /content/colB5BERT\n",
        "!git pull\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8evBd6sMsqT",
        "outputId": "d9cd27e5-7dc1-4e4c-af07-7ba7ae0ab18c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'colB5BERT' already exists and is not an empty directory.\n",
            "/content/colB5BERT\n",
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FneXYJMlY6-K"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import textwrap\n",
        "import pickle\n",
        "import h5py\n",
        "import logging\n",
        "\n",
        "from transformers import BertModel, BertTokenizer\n",
        "from transformers import RobertaModel, RobertaTokenizer\n",
        "\n",
        "import torch\n",
        "from torch.nn.functional import cosine_similarity\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOgZt0MKWbOI"
      },
      "source": [
        "Modules `vsm`, `utils` and `sst` are from Stanford's CS224u https://github.com/cgpotts/cs224u"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wIOgkGQmkk-5"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/colB5BERT/python/')\n",
        "\n",
        "import utils\n",
        "import vsm\n",
        "import sst\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "JAqaTuEMVTsp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c170c9b0-2507-49d1-a758-b6af527e8746"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Jun 20 01:36:54 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    43W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dTrPrG2kdCz"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pboznS998H17",
        "outputId": "d7e6cad0-0880-4cf3-9f31-832b7808ee33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 11537 entries, 0 to 11536\n",
            "Data columns (total 3 columns):\n",
            " #   Column          Non-Null Count  Dtype \n",
            "---  ------          --------------  ----- \n",
            " 0   id              11537 non-null  int64 \n",
            " 1   id_divisao      11537 non-null  int64 \n",
            " 2   texto_dividido  11537 non-null  object\n",
            "dtypes: int64(2), object(1)\n",
            "memory usage: 270.5+ KB\n"
          ]
        }
      ],
      "source": [
        "b5_data = pd.read_csv('/content/colB5BERT/data/db_textos.splitted.csv', sep=';')\n",
        "b5_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b5_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "ZX_oY24kO-Tg",
        "outputId": "6a6f2163-df7a-4915-ee7a-63e61eb9f95b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id  id_divisao                                     texto_dividido\n",
              "0      100           1  ajudando porque a zuzu é um amor e tem a voz f...\n",
              "1      100           2  vai ter share sim e se reclamar dou share mais...\n",
              "2      100           3  quanto parece , A , $NUMBER$ . MvC $NUMBER$ CL...\n",
              "3      100           4  $NUMBER$ jeitos de dar entry então é só sucess...\n",
              "4      100           5  quiser se vira Esse livro é de co-autoria de $...\n",
              "...    ...         ...                                                ...\n",
              "11532  999           6  amo muito ! < $NUMBER$ < $NUMBER$ \" Fique por ...\n",
              "11533  999           7  pai ! Feliz aniversário ! < $NUMBER$ < $NUMBER...\n",
              "11534  999           8  rei do $NAME$ Club de $NAME$ Oeste : $NAME$ $N...\n",
              "11535  999           9  todo tipo de público . A realização do projeto...\n",
              "11536  999          10  sobre sua vida , e as mais recentes ajudam a c...\n",
              "\n",
              "[11537 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2b46f23d-ba83-4f95-bc9f-b7f510a79f41\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>id_divisao</th>\n",
              "      <th>texto_dividido</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100</td>\n",
              "      <td>1</td>\n",
              "      <td>ajudando porque a zuzu é um amor e tem a voz f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>100</td>\n",
              "      <td>2</td>\n",
              "      <td>vai ter share sim e se reclamar dou share mais...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100</td>\n",
              "      <td>3</td>\n",
              "      <td>quanto parece , A , $NUMBER$ . MvC $NUMBER$ CL...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>100</td>\n",
              "      <td>4</td>\n",
              "      <td>$NUMBER$ jeitos de dar entry então é só sucess...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>100</td>\n",
              "      <td>5</td>\n",
              "      <td>quiser se vira Esse livro é de co-autoria de $...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11532</th>\n",
              "      <td>999</td>\n",
              "      <td>6</td>\n",
              "      <td>amo muito ! &lt; $NUMBER$ &lt; $NUMBER$ \" Fique por ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11533</th>\n",
              "      <td>999</td>\n",
              "      <td>7</td>\n",
              "      <td>pai ! Feliz aniversário ! &lt; $NUMBER$ &lt; $NUMBER...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11534</th>\n",
              "      <td>999</td>\n",
              "      <td>8</td>\n",
              "      <td>rei do $NAME$ Club de $NAME$ Oeste : $NAME$ $N...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11535</th>\n",
              "      <td>999</td>\n",
              "      <td>9</td>\n",
              "      <td>todo tipo de público . A realização do projeto...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11536</th>\n",
              "      <td>999</td>\n",
              "      <td>10</td>\n",
              "      <td>sobre sua vida , e as mais recentes ajudam a c...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11537 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2b46f23d-ba83-4f95-bc9f-b7f510a79f41')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2b46f23d-ba83-4f95-bc9f-b7f510a79f41 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2b46f23d-ba83-4f95-bc9f-b7f510a79f41');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "base_itens_b5 = pd.read_excel('/content/colB5BERT/data/base_itens.xlsx')\n",
        "\n",
        "\n",
        "base_itens_b5\n",
        "base_itens_b5.info()\n",
        "# base_itens_b5['item_pt_text'].tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOaFKAKmhp5m",
        "outputId": "28c05099-1de5-4419-f8a6-e6e131b8e3db"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 415 entries, 0 to 414\n",
            "Data columns (total 9 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   ord0_index    415 non-null    int64  \n",
            " 1   test          415 non-null    object \n",
            " 2   coditem       415 non-null    object \n",
            " 3   item_pt_text  415 non-null    object \n",
            " 4   item_en_text  415 non-null    object \n",
            " 5   domain        413 non-null    object \n",
            " 6   facet         413 non-null    object \n",
            " 7   pole          415 non-null    int64  \n",
            " 8   seman_pairs   273 non-null    float64\n",
            "dtypes: float64(1), int64(2), object(6)\n",
            "memory usage: 29.3+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JS59UBqrY6-L"
      },
      "source": [
        "### Loading Transformer models\n",
        "Specify a model, a tokenizer, and load a model pretrained weights:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "O6f6UXqGY6-L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71267e86-2eea-4503-d530-f32030f174cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "bert_weights_name = 'neuralmind/bert-base-portuguese-cased'\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(bert_weights_name)\n",
        "bert_model = BertModel.from_pretrained(bert_weights_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHrst3TZY6-M"
      },
      "source": [
        "### The basics of tokenizing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOg05YkzY6-M"
      },
      "outputs": [],
      "source": [
        "def print_cell(df, row, column, wrap_length=80):\n",
        "    if row < len(df) and column in df.columns:\n",
        "        text = df.loc[row, column]\n",
        "        print('\\n'.join(textwrap.wrap(text, width=wrap_length)))\n",
        "    else:\n",
        "        print(\"Invalid row or column\")\n",
        "\n",
        "print_cell(b5_data, 3, 'texto_dividido')\n",
        "\n",
        "ex_ids = bert_tokenizer.encode(b5_data.loc[3, 'texto_dividido'], add_special_tokens=True)\n",
        "bert_tokenizer.convert_ids_to_tokens(ex_ids)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDRVTaxRY6-P"
      },
      "source": [
        "### Getting BERT embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wV8TNuFDY6-P"
      },
      "source": [
        "To obtain the representations for a batch of examples, we use the `forward` method of the model, as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9BLypVjY6-P"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    reps = bert_model(torch.tensor([ex_ids]), output_hidden_states=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_texts(texts):\n",
        "    # Tokenize each text and convert to input IDs\n",
        "    input_ids = [bert_tokenizer.encode(text, add_special_tokens=True) for text in texts]\n",
        "    return input_ids\n",
        "\n",
        "\n",
        "def tokenize_texts(bert_tokenizer, texts):\n",
        "    tokenized_texts = []\n",
        "    for text in texts:\n",
        "        encoded_text = bert_tokenizer.encode(text, add_special_tokens=True)\n",
        "        # truncate the encoded text to the first 512 tokens\n",
        "        encoded_text = encoded_text[:512]\n",
        "        # encoded_text = encoded_text\n",
        "        tokenized_texts.append(encoded_text)\n",
        "    return tokenized_texts\n",
        "\n"
      ],
      "metadata": {
        "id": "r11vxToiVrfP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize_texts(ex_of_texts)\n",
        "ex_of_texts = b5_data.iloc[[0, 1, 2, 3], b5_data.columns.get_loc('texto_dividido')].tolist()\n",
        "lengths = [len(sublist) for sublist in tokenize_texts(bert_tokenizer, ex_of_texts)]\n",
        "\n",
        "print(lengths)  # Output: [3, 2, 4]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2ddFICvXD7F",
        "outputId": "081ed75e-be17-4b9a-cafa-75298797954a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[512, 512, 477, 512]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bert_embeddings(bert_model, examples, layers):\n",
        "    # Move model to GPU if available\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    bert_model = bert_model.to(device)\n",
        "\n",
        "    embeddings = {layer: [] for layer in layers}\n",
        "    for ex_ids in examples:\n",
        "        # Convert data to tensor and move to GPU\n",
        "        ex_ids_tensor = torch.tensor([ex_ids]).to(device)\n",
        "        with torch.no_grad():\n",
        "            # Output includes 'last_hidden_state', 'pooler_output', 'hidden_states'\n",
        "            output = bert_model(ex_ids_tensor, output_hidden_states=True)\n",
        "            hidden_states = output.hidden_states\n",
        "            for layer in layers:\n",
        "                # Verify layer index is valid\n",
        "                if layer < 0 or layer >= len(hidden_states):\n",
        "                    print(f\"Invalid layer {layer}\")\n",
        "                else:\n",
        "                    # Hidden states is a tuple. Indexing into it gives a tensor of shape\n",
        "                    # (batch_size, sequence_length, hidden_size). Since batch_size is 1,\n",
        "                    # we remove the batch dimension.\n",
        "                    layer_output = hidden_states[layer].squeeze(0)\n",
        "                    # Convert back to CPU for further processing or storage\n",
        "                    embeddings[layer].append(layer_output.to('cpu'))\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "6uybcKQ_VFQO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layers = [6, 9, 11, 12]  # Specify the layers you want\n",
        "layers = [6]\n",
        "embeddings = get_bert_embeddings(bert_model, tokenize_texts(bert_tokenizer, ex_of_texts), layers)\n",
        "\n",
        "import pprint\n",
        "pprint.pprint(embeddings )\n",
        "\n",
        "\n",
        "dir(embeddings)\n",
        "vars(embeddings)\n",
        "import inspect\n",
        "inspect.getmembers(embeddings)\n",
        "\n",
        "pprint.pprint(embeddings[9])\n",
        "len(embeddings[9])\n",
        "len(embeddings[9][2])\n",
        "pprint.pprint(embeddings[9][2])\n",
        "x = embeddings[9][2]\n",
        "x.shape\n",
        "\n",
        "dimensions = [len(inner_list) for inner_list in embeddings[9][0]]\n",
        "\n"
      ],
      "metadata": {
        "id": "cpAtS9ZIVH8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finally getting the embeddings"
      ],
      "metadata": {
        "id": "k4pHrJ-3Du2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the layers you want\n",
        "layers = [6, 9, 11, 12]\n",
        "layers = [6]\n",
        "\n",
        "layers = [11]\n",
        "\n",
        "len(b5_data['texto_dividido'].tolist())\n",
        "\n",
        "embeddings_posts = get_bert_embeddings(bert_model, tokenize_texts(bert_tokenizer, b5_data['texto_dividido'].tolist()), layers)\n",
        "embeddings_itens = get_bert_embeddings(bert_model, tokenize_texts(bert_tokenizer, base_itens_b5['item_pt_text'].tolist()), layers)\n",
        "\n"
      ],
      "metadata": {
        "id": "l50Z1laf-fof"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save embeddings\n",
        "This doesn't work. Post embeedings is 56Gb"
      ],
      "metadata": {
        "id": "SAJyN9gE3Juk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_embeddings_to_disk(embeddings, filename):\n",
        "    # Convert tensors to numpy arrays and store them in the same structure\n",
        "    numpy_embeddings = {str(layer): [t.numpy() for t in tensors] for layer, tensors in embeddings.items()}\n",
        "\n",
        "    # Use numpy's savez function to store the dictionary\n",
        "    # We use ** to unpack the dictionary into keyword arguments\n",
        "    np.savez(filename, **numpy_embeddings)\n",
        "\n",
        "def load_embeddings_from_disk(filename):\n",
        "    with np.load(filename) as data:\n",
        "        embeddings = {layer: data[layer] for layer in data.files}\n",
        "    return embeddings\n",
        "\n",
        "filename=\"/content/drive/MyDrive/colB5BERT/embeddings_itens\"\n",
        "\n",
        "save_embeddings_to_disk(embeddings=embeddings_itens, filename=filename)\n",
        "\n",
        "filename=\"/content/drive/MyDrive/colB5BERT/embeddings_posts\"\n",
        "save_embeddings_to_disk(embeddings=embeddings_posts, filename=filename)\n"
      ],
      "metadata": {
        "id": "xzFTFdsti-2G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "327c4cbe-d39a-46b4-da80-1c495604cae0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py:713: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  val = np.asanyarray(val)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embeddings is a dict of `layers` keys. Each component of one key is composed of `batch size` elments of a tensor with size `num_of_tokens X embedding_dim`. The final structure is `layers X batch size X num_of_tokens X embedding_dim`"
      ],
      "metadata": {
        "id": "t4aDJx91AehR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# how many posts\n",
        "len(embeddings_posts[6])\n",
        "embeddings_posts[6]\n",
        "\n",
        "# what is the first element ? of the first post\n",
        "type(embeddings_posts[6][0])\n",
        "\n",
        "# what is the dimension\n",
        "embeddings_posts[6][3].shape\n",
        "\n",
        "\n",
        "# how many posts\n",
        "len(embeddings_itens[6])\n",
        "\n",
        "# what is the first element ? of the first post\n",
        "type(embeddings_itens[6][0])\n",
        "\n",
        "# what is the dimension\n",
        "embeddings_itens[6][0].shape\n",
        "\n",
        "type(embeddings_itens[6])"
      ],
      "metadata": {
        "id": "H1d5NWmsDnVo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17cb876c-db89-432f-82e4-d58f4441f57e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([512, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cosine Similarity\n",
        "\n",
        "Testing the function"
      ],
      "metadata": {
        "id": "e8R8O1Jf3dP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_cosine_similarity0(list_A, list_B):\n",
        "    result = []\n",
        "    for tensor_A in list_A:\n",
        "        for tensor_B in list_B:\n",
        "            # Expand dimensions so that shapes are [n_tokens_A, 1, 768] and [1, n_tokens_B, 768]\n",
        "            tensor_A_exp = tensor_A.unsqueeze(1)\n",
        "            tensor_B_exp = tensor_B.unsqueeze(0)\n",
        "            # Compute cosine similarity\n",
        "            similarity = cosine_similarity(tensor_A_exp, tensor_B_exp, dim=-1)\n",
        "            result.append(similarity)\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "7PIs8Nsp4GHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then use cosine_similarity(tensor_A_exp, tensor_B_exp, dim=-1) to compute the cosine similarity between all pairs of tokens from tensor_A and tensor_B, resulting in a tensor of shape [n_tokens_A, n_tokens_B]. Each element (i,j) in the resulting tensor represents the cosine similarity between the ith token from tensor_A and the jth token from tensor_B.\n",
        "\n"
      ],
      "metadata": {
        "id": "Wu2VNlfL40Q6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp = calculate_cosine_similarity0(embeddings_itens[6][0:10], embeddings_posts[6][0:5])\n",
        "\n",
        "temp.shape\n",
        "len(temp) # 10 itens X 5 posts\n",
        "len(temp[5][2]) # Item 1 com 13 tokens\n",
        "len(temp[0][9])\n",
        "temp[0].shape\n",
        "temp2 = torch.cat(temp.unsqueeze(0), dim=-1)"
      ],
      "metadata": {
        "id": "GT4_xNOi4H30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code tryes to recover all cosine similarities. But explode RAM"
      ],
      "metadata": {
        "id": "GtZYO8yZTmeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# user = os.getenv('DB_USER')\n",
        "# password = os.getenv('DB_PASS')\n",
        "!pip install PyMySQL\n",
        "import pymysql\n",
        "\n",
        "# Establish the connection\n",
        "conn = pymysql.connect(\n",
        "    host = os.getenv('LABAPE6_HOST'),\n",
        "    user = os.getenv('LABAPE6_USER'),\n",
        "    password = os.getenv('LABAPE6_PASSWORD'),\n",
        "    db = os.getenv('LABAPE6_DBNAME')\n",
        ")\n",
        "\n",
        "# Create a cursor\n",
        "cur = conn.cursor()\n",
        "\n",
        "# Execute a query to fetch data\n",
        "cur.execute(\"SELECT * FROM your_table\")\n",
        "\n",
        "# Fetch the results\n",
        "results = cur.fetchall()\n",
        "\n",
        "for row in results:\n",
        "    print(row)\n",
        "\n",
        "# Now, let's insert data into a table\n",
        "cur.execute(\"INSERT INTO your_table (col1, col2) VALUES ('data1', 'data2')\")\n",
        "\n",
        "# Commit your changes\n",
        "conn.commit()\n",
        "\n",
        "# Close the connection\n",
        "conn.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "oHPJBaIFk1Tx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(level=logging.INFO)  # you can change the level to DEBUG if you want more detailed logs\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def calculate_cosine_similarity1(list_A, list_B, batch_size=1):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    result = []\n",
        "\n",
        "    # Move list_A tensors to GPU\n",
        "    list_A = [tensor.to(device) for tensor in list_A]\n",
        "\n",
        "    # Process batch_size tensor_A at a time\n",
        "    for i in tqdm(range(0, len(list_A), batch_size), desc='Processing', dynamic_ncols=True):\n",
        "        batch_A = list_A[i:i+batch_size]\n",
        "        batch_A = torch.nn.utils.rnn.pad_sequence(batch_A, batch_first=True).unsqueeze(2)  # [batch_size, max_n_tokens_A, 1, 768]\n",
        "        similarities = []\n",
        "\n",
        "        for tensor_B in list_B:\n",
        "            tensor_B = tensor_B.to(device).unsqueeze(0).unsqueeze(0)  # [1, 1, n_tokens_B, 768]\n",
        "            similarity = torch.nn.functional.cosine_similarity(batch_A, tensor_B, dim=-1)  # [batch_size, max_n_tokens_A, n_tokens_B]\n",
        "            similarity = similarity.half()  # Reduce precision to float16\n",
        "            similarities.append(similarity.cpu().numpy())  # Move similarity to CPU and convert to NumPy array\n",
        "            del tensor_B  # Delete tensor_B from GPU memory\n",
        "\n",
        "            # Store the batch's similarities to the result\n",
        "            result.append(similarities)\n",
        "\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "ar4CrkpS78Nc"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosim_L11 = calculate_cosine_similarity1(embeddings_posts[11], embeddings_itens[11], batch_size=1)\n",
        "\n",
        "\n",
        "import h5py\n",
        "\n",
        "def save_to_hdf5(data, filename, dataset_name):\n",
        "    with h5py.File(filename, 'w') as hf:\n",
        "        hf.create_dataset(dataset_name, data=data)\n",
        "\n",
        "\n",
        "filname=\"/content/drive/MyDrive/colB5BERT/cosim_L11.hd5\"\n",
        "\n",
        "save_to_hdf5(cosim_L11, filname, 'cosim_data')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXF0_Sd2lkeY",
        "outputId": "c7cde07c-205b-44c9-e07e-c6e170b5ad27"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing: 100%|██████████| 11537/11537 [19:22<00:00,  9.93it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Determine the size of each chunk\n",
        "CHUNK_SIZE = 11 * 11537  # This is an example, adjust based on your data and system capacity\n",
        "\n",
        "# Load your large data\n",
        "#with open('/content/drive/MyDrive/colB5BERT/cosim_L11.pkl', 'rb') as f:\n",
        "#    cosim_L11 = pickle.load(f)\n",
        "\n",
        "# Determine the number of chunks\n",
        "num_elements = len(cosim_L11)\n",
        "NUM_CHUNKS = num_elements // CHUNK_SIZE\n",
        "if num_elements % CHUNK_SIZE:\n",
        "    NUM_CHUNKS += 1\n",
        "\n",
        "# Save each chunk to a separate file\n",
        "for i in range(NUM_CHUNKS):\n",
        "    start = i * CHUNK_SIZE\n",
        "    end = min((i + 1) * CHUNK_SIZE, num_elements)\n",
        "    chunk = cosim_L11[start:end]\n",
        "\n",
        "    with open(f'/content/drive/MyDrive/colB5BERT/cosim_L11_chunk_{i}.pkl', 'wb') as f:\n",
        "        pickle.dump(chunk, f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYl9HAbKyha1",
        "outputId": "752555c3-a712-4738-9852-f2e617ddfda4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "38"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mysql-connector-python\n",
        "import mysql.connector\n",
        "\n",
        "\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)  # you can change the level to DEBUG if you want more detailed logs\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def calculate_cosine_similarity1_db(list_A, list_B, db_config, table_name, batch_size=1):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Move list_A tensors to GPU\n",
        "    list_A = [tensor.to(device) for tensor in list_A]\n",
        "\n",
        "    # Connect to the MySQL server\n",
        "    cnx = mysql.connector.connect(**db_config)\n",
        "    cursor = cnx.cursor()\n",
        "\n",
        "    # Create a new table\n",
        "    cursor.execute(f\"CREATE TABLE IF NOT EXISTS {table_name} (n_batch INT, num_element_listA INT, num_token_A INT, num_element_list_B INT, num_token_B INT, cos_sim FLOAT)\")\n",
        "\n",
        "    # Process batch_size tensor_A at a time\n",
        "    for n_batch in tqdm(range(0, len(list_A), batch_size), desc='Processing', dynamic_ncols=True):\n",
        "        batch_A = list_A[n_batch:n_batch+batch_size]\n",
        "        batch_A = torch.nn.utils.rnn.pad_sequence(batch_A, batch_first=True).unsqueeze(2)  # [batch_size, max_n_tokens_A, 1, 768]\n",
        "        batch_data = []\n",
        "\n",
        "        for num_element_list_B, tensor_B in enumerate(list_B):\n",
        "            tensor_B = tensor_B.to(device).unsqueeze(0).unsqueeze(0)  # [1, 1, n_tokens_B, 768]\n",
        "            similarity = torch.nn.functional.cosine_similarity(batch_A, tensor_B, dim=-1)  # [batch_size, max_n_tokens_A, n_tokens_B]\n",
        "            similarity = similarity.half()  # Reduce precision to float16\n",
        "            similarity_np = similarity.cpu().numpy()  # Move similarity to CPU and convert to NumPy array\n",
        "\n",
        "            # Iterate over the elements in the similarity matrix\n",
        "            for num_element_listA in range(similarity_np.shape[0]):\n",
        "                for num_token_A in range(similarity_np.shape[1]):\n",
        "                    for num_token_B in range(similarity_np.shape[2]):\n",
        "                        # Extract the corresponding cosine similarity\n",
        "                        cos_sim = similarity_np[num_element_listA, num_token_A, num_token_B]\n",
        "\n",
        "                        # Add the row to the batch_data list\n",
        "                        batch_data.append((n_batch, num_element_listA, num_token_A, num_element_list_B, num_token_B, float(cos_sim)))\n",
        "\n",
        "            del tensor_B  # Delete tensor_B from GPU memory\n",
        "\n",
        "        # Insert all rows in batch_data into the database\n",
        "        query = f\"INSERT INTO {table_name} (n_batch, num_element_listA, num_token_A, num_element_list_B, num_token_B, cos_sim) VALUES (%s, %s, %s, %s, %s, %s)\"\n",
        "        cursor.executemany(query, batch_data)\n",
        "\n",
        "        logger.info(f\"Inserted {len(batch_data)} rows into the database\")\n",
        "\n",
        "    # Make sure data is committed to the database\n",
        "    cnx.commit()\n",
        "    cursor.close()\n",
        "    cnx.close()\n",
        "\n",
        "# Example usage\n",
        "db_config = {'host': 'localhost', 'database': 'test_db', 'user': 'test_user', 'password': 'test_password'}\n",
        "calculate_cosine_similarity1_db(list_A, list_B, db_config=db_config, table\n",
        "\n"
      ],
      "metadata": {
        "id": "du4qj1DApf0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9e29dYSxwnvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# print(batch)\n",
        "# print(element_A)\n",
        "# print(batch_A.shape[1])\n",
        "# print(element_B)\n",
        "# print(tensor_B.shape[2])\n",
        "# print(similarity[element_A])\n",
        "\n",
        "calculate_cosine_similarity1_db(embeddings_itens[6], embeddings_posts[6], db_config=db_config,table_name=\"cos_sim_L6\",  batch_size=1)\n"
      ],
      "metadata": {
        "id": "eUsAXqmSrWBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(level=logging.INFO)  # you can change the level to DEBUG if you want more detailed logs\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def calculate_cosine_similarity1_hdf5b(list_A, list_B, filename, dataset_name, max_tokens_A, max_tokens_B, num_batches, num_list_A, num_list_B, batch_size=1):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Move list_A tensors to GPU\n",
        "    list_A = [tensor.to(device) for tensor in list_A]\n",
        "\n",
        "    # Create a new HDF5 file\n",
        "    with h5py.File(filename, \"w\") as f:\n",
        "        # Create the dataset\n",
        "        dset = f.create_dataset(dataset_name, (num_batches, num_list_A, num_list_B, max_tokens_A, max_tokens_B), dtype='float16')\n",
        "\n",
        "        # Process batch_size tensor_A at a time\n",
        "        for n_batch in tqdm(range(0, len(list_A), batch_size), desc='Processing', dynamic_ncols=True):\n",
        "            batch_A = list_A[n_batch:n_batch+batch_size]\n",
        "            batch_A = torch.nn.utils.rnn.pad_sequence(batch_A, batch_first=True).unsqueeze(2)  # [batch_size, max_n_tokens_A, 1, 768]\n",
        "\n",
        "            for n_listA_elem, tensor_A in enumerate(batch_A):\n",
        "                for n_listB_elem, tensor_B in enumerate(list_B):\n",
        "                    tensor_B = tensor_B.to(device).unsqueeze(0).unsqueeze(0)  # [1, 1, n_tokens_B, 768]\n",
        "                    similarity = torch.nn.functional.cosine_similarity(tensor_A.unsqueeze(0), tensor_B, dim=-1)  # [1, n_tokens_A, n_tokens_B]\n",
        "                    similarity = similarity.half()  # Reduce precision to float16\n",
        "                    similarity_np = similarity.cpu().numpy()  # Move similarity to CPU and convert to NumPy array\n",
        "\n",
        "                    # Write the similarities to the HDF5 dataset\n",
        "                    dset[n_batch, n_listA_elem, n_listB_elem, :similarity_np.shape[1], :similarity_np.shape[2]] = similarity_np\n",
        "\n",
        "                    del tensor_B  # Delete tensor_B from GPU memory\n",
        "                    logger.info(f\"batch: {n_batch}, A elem: {n_listA_elem}, B elem: {n_listB_elem} , similarity shape: {similarity_np.shape[1]} X {similarity_np.shape[2]}\")\n"
      ],
      "metadata": {
        "id": "Ms010bDDVFai"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mcuNj_KVchHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#cosim_top5_L6 = calculate_cosine_similarity2(embeddings_itens[6][0:3], embeddings_posts[6][0:10], topk = 5, batch_size=2)\n",
        "filname=\"cosim_L11\"\n",
        "dataset_name = \"db_cosim_L1\"\n",
        "max_tokens_A = 512\n",
        "max_tokens_B = 23\n",
        "num_batches = len(embeddings_posts[11])\n",
        "num_list_A = len(embeddings_posts[11])\n",
        "num_list_B = len(embeddings_itens[11])\n",
        "batch_size=1\n",
        "\n",
        "calculate_cosine_similarity1_hdf5b(embeddings_posts[11], embeddings_itens[11], filname, dataset_name, max_tokens_A, max_tokens_B, num_batches, num_list_A, num_list_B, batch_size)"
      ],
      "metadata": {
        "id": "H9fzJ176bRBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "def calculate_cosine_similarity1_hdf5(list_A, list_B, filename, dataset_name, batch_size=1):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Move list_A tensors to GPU\n",
        "    list_A = [tensor.to(device) for tensor in list_A]\n",
        "\n",
        "    # Create a new HDF5 file\n",
        "    with h5py.File(filename, \"w\") as f:\n",
        "        # Process batch_size tensor_A at a time\n",
        "        for n_batch in tqdm(range(0, len(list_A), batch_size), desc='Processing', dynamic_ncols=True):\n",
        "            batch_A = list_A[n_batch:n_batch+batch_size]\n",
        "            batch_A = torch.nn.utils.rnn.pad_sequence(batch_A, batch_first=True).unsqueeze(2)  # [batch_size, max_n_tokens_A, 1, 768]\n",
        "\n",
        "            for num_element_list_B, tensor_B in enumerate(list_B):\n",
        "                tensor_B = tensor_B.to(device).unsqueeze(0).unsqueeze(0)  # [1, 1, n_tokens_B, 768]\n",
        "                similarity = torch.nn.functional.cosine_similarity(batch_A, tensor_B, dim=-1)  # [batch_size, max_n_tokens_A, n_tokens_B]\n",
        "                similarity = similarity.half()  # Reduce precision to float16\n",
        "                similarity_np = similarity.cpu().numpy()  # Move similarity to CPU and convert to NumPy array\n",
        "\n",
        "                # Create a new dataset for each batch, element of list_A, and element of list_B\n",
        "                dset = f.create_dataset(f\"{dataset_name}/batch_{n_batch}_A_{num_element_list_B}_B\", data=similarity_np, dtype='float16')\n",
        "\n",
        "                del tensor_B  # Delete tensor_B from GPU memory\n"
      ],
      "metadata": {
        "id": "OavwvwsKXLEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aASSJJmuhWX6",
        "outputId": "f76a6b59-1770-4f6a-fb54-64f6678e3d42"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 24K\n",
            "-rw-r--r-- 1 root root  282 Jun 19 15:41 colB5BERT.Rproj\n",
            "drwxr-xr-x 2 root root 4.0K Jun 19 15:41 data\n",
            "drwxr-xr-x 2 root root 4.0K Jun 19 15:41 docs\n",
            "drwxr-xr-x 3 root root 4.0K Jun 19 15:42 python\n",
            "drwxr-xr-x 2 root root 4.0K Jun 19 15:41 R\n",
            "-rw-r--r-- 1 root root  103 Jun 19 15:41 README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code saves the Top K cosine similarities with text divided tokens with all tokens in all items"
      ],
      "metadata": {
        "id": "KNsrI8nN6LnI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_cosine_similarity2(list_A, list_B, topk=5, batch_size=10):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    result = []\n",
        "\n",
        "    # Move list_A tensors to GPU\n",
        "    list_A = [tensor.to(device) for tensor in list_A]\n",
        "\n",
        "    # Process batch_size tensor_A at a time\n",
        "    for i in tqdm(range(0, len(list_A), batch_size), desc='Processing', dynamic_ncols=True):\n",
        "        batch_A = list_A[i:i+batch_size]\n",
        "        batch_A = torch.nn.utils.rnn.pad_sequence(batch_A, batch_first=True).unsqueeze(2)  # [batch_size, max_n_tokens_A, 1, 768]\n",
        "\n",
        "        for tensor_B in list_B:\n",
        "            tensor_B = tensor_B.to(device).unsqueeze(0).unsqueeze(0)  # [1, 1, n_tokens_B, 768]\n",
        "            similarity = torch.nn.functional.cosine_similarity(batch_A, tensor_B, dim=-1)  # [batch_size, max_n_tokens_A, n_tokens_B]\n",
        "\n",
        "             # [batch_size, max_n_tokens_A, n_tokens_B]\n",
        "\n",
        "            current_topk = min(topk, similarity.size(-1))  # Ensure topk isn't larger than the number of tokens in tensor_B\n",
        "\n",
        "            topk_values, topk_indices = torch.topk(similarity, current_topk, dim=-1)  # [batch_size, max_n_tokens_A, current_topk]\n",
        "            result.append((topk_values.cpu().numpy(), topk_indices.cpu().numpy()))  # Move topk_values and topk_indices to CPU\n",
        "            del tensor_B  # Delete tensor_B from GPU memory\n",
        "\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "a_bRt94M0qis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trying to get embeddings in batched slices"
      ],
      "metadata": {
        "id": "KOqWEkAD1rsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_cosine_similarity3(list_A, posts=b5_data['texto_dividido'].tolist(), topk=5, batch_size=25, layer=6):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    result = []\n",
        "\n",
        "\n",
        "    # Move list_A tensors to GPU\n",
        "    list_A = [tensor.to(device) for tensor in list_A]\n",
        "\n",
        "    # Process batch_size tensor_A at a time\n",
        "    for i in tqdm(range(0, len(posts), batch_size), desc='Processing', dynamic_ncols=True):\n",
        "        # batch_A = list_A[i:i+batch_size]\n",
        "        list_A = torch.nn.utils.rnn.pad_sequence(list_A, batch_first=True).unsqueeze(2)  # [batch_size, max_n_tokens_A, 1, 768]\n",
        "\n",
        "\n",
        "        batch_posts = get_bert_embeddings(bert_model, tokenize_texts(bert_tokenizer, posts[i:i+batch_size]), layers=[layer])\n",
        "\n",
        "        for tensor_B in batch_posts[layer]:\n",
        "            tensor_B = tensor_B.to(device).unsqueeze(0).unsqueeze(0)  # [1, 1, n_tokens_B, 768]\n",
        "            similarity = torch.nn.functional.cosine_similarity(list_A, tensor_B, dim=-1)  # [batch_size, max_n_tokens_A, n_tokens_B]\n",
        "\n",
        "            current_topk = min(topk, similarity.size(-1))  # Ensure topk isn't larger than the number of tokens in tensor_B\n",
        "\n",
        "            topk_values, topk_indices = torch.topk(similarity, current_topk, dim=-1)  # [batch_size, max_n_tokens_A, current_topk]\n",
        "            result.append((topk_values.cpu(), topk_indices.cpu()))  # Move topk_values and topk_indices to CPU\n",
        "            del tensor_B  # Delete tensor_B from GPU memory\n",
        "\n",
        "    return result\n",
        "\n"
      ],
      "metadata": {
        "id": "nPIosPJpTsyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `result` is a list of tuples, where each tuple consists of `topk_values` and `topk_indices`.\n",
        "\n",
        "1. `topk_values` is a PyTorch tensor of shape `[batch_size, max_n_tokens_A, current_topk]`, where `batch_size` is the number of `tensor_A` processed at a time, `max_n_tokens_A` is the maximum number of tokens in `tensor_A` in the current batch, and `current_topk` is the number of top similarity scores to return, which is either the `topk` parameter or the number of tokens in `tensor_B`, whichever is smaller. This tensor holds the `current_topk` highest cosine similarity scores for each token in each `tensor_A` in the batch against `tensor_B`.\n",
        "\n",
        "2. `topk_indices` is a PyTorch tensor of the same shape as `topk_values` (`[batch_size, max_n_tokens_A, current_topk]`). This tensor holds the indices of the `current_topk` highest cosine similarity scores in `tensor_B`.\n",
        "\n",
        "So, in `result`, you will have a list of tuples (each for one `tensor_B`) and each tuple contains two tensors (`topk_values` and `topk_indices`) of shape `[batch_size, max_n_tokens_A, current_topk]`. As the function processes `list_A` in batches, the exact length of `result` will be `len(list_A) / batch_size * len(list_B)` (if `len(list_A)` is exactly divisible by `batch_size`), or `len(list_A) // batch_size * len(list_B) + len(list_B)` (if it's not)."
      ],
      "metadata": {
        "id": "6SQlC6I8d4-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cosim_top5_L6 = calculate_cosine_similarity2(embeddings_itens[6][0:3], embeddings_posts[6][0:10], topk = 5, batch_size=2)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "22Rwhgf_AGFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(cosim_top5_L6)\n",
        "\n",
        "embeddings_itens[6][1].shape\n",
        "cosim_top5_L6[0][0].shape\n",
        "\n",
        "\n",
        "len(embeddings_L6)\n",
        "batch_size = 5\n",
        "len(embeddings_L6) == len(list(range(0, len(embeddings_itens[6]), batch_size))) * len(embeddings_posts[6])\n",
        "\n",
        "len(embeddings_L6[0])\n",
        "len(embeddings_L6[0][:])\n",
        "len()\n",
        "\n",
        "\n",
        "[ print(x) for x in embeddings_L6[0][0] ]\n",
        "\n",
        "temp = embeddings_L6[0][0]\n",
        "len(temp[0][412])\n",
        "temp1 = temp[512]\n",
        "temp1[412]\n",
        "\n",
        "temp1 = temp[412][412]\n",
        "\n",
        "len(temp1[5])\n",
        "for i in range(2):  # Change this number to see more/less\n",
        "    topk_values, topk_indices = embeddings_L6[i]\n",
        "    print(\"TopK Values Tensor:\")\n",
        "    print(topk_values)\n",
        "    print(\"TopK Indices Tensor:\")\n",
        "    print(topk_indices)\n",
        "    print(\"\\n---\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izlQ9PJ2c_Lx",
        "outputId": "4bdabd63-7fb6-4c4e-98f3-2b77f124cb10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0.9999438 , 0.28923154, 0.16323651, 0.15610822, 0.15447576],\n",
              "        [0.54535484, 0.5104598 , 0.50956935, 0.5010768 , 0.49993923],\n",
              "        [0.6487386 , 0.6451509 , 0.64438605, 0.5922517 , 0.58099186],\n",
              "        [0.5901557 , 0.58609843, 0.5840987 , 0.5761634 , 0.5509137 ],\n",
              "        [0.48015648, 0.4667238 , 0.46271023, 0.45601392, 0.45178825],\n",
              "        [0.5916519 , 0.56757987, 0.55520004, 0.5527642 , 0.5513735 ],\n",
              "        [0.65486205, 0.65253234, 0.6455029 , 0.64238495, 0.6225333 ],\n",
              "        [0.49928924, 0.49720046, 0.47456792, 0.46936598, 0.46704215],\n",
              "        [0.70637774, 0.656662  , 0.60018235, 0.5642161 , 0.5619816 ],\n",
              "        [0.46530312, 0.4589913 , 0.45810091, 0.45756498, 0.45258853],\n",
              "        [0.6122791 , 0.60375166, 0.5908047 , 0.5653817 , 0.5639473 ],\n",
              "        [0.44506344, 0.44214535, 0.44137588, 0.43476218, 0.43126088],\n",
              "        [0.56115127, 0.5407094 , 0.5342648 , 0.5315968 , 0.527797  ]],\n",
              "\n",
              "       [[0.999931  , 0.2896551 , 0.16347511, 0.15663569, 0.15498869],\n",
              "        [0.5109098 , 0.4984308 , 0.47713146, 0.47695607, 0.475707  ],\n",
              "        [0.5885464 , 0.53449386, 0.53435004, 0.5322015 , 0.52729815],\n",
              "        [0.39510918, 0.39323214, 0.39226165, 0.38135108, 0.3765064 ],\n",
              "        [0.74875015, 0.7224337 , 0.66609323, 0.6006621 , 0.5642407 ],\n",
              "        [0.61165833, 0.57550144, 0.55801725, 0.5573061 , 0.53537   ],\n",
              "        [0.65990055, 0.6312597 , 0.6193184 , 0.58876604, 0.57877314],\n",
              "        [0.45700127, 0.454912  , 0.4496091 , 0.4416232 , 0.44085503],\n",
              "        [0.56337935, 0.53748786, 0.53411883, 0.5276868 , 0.52570945],\n",
              "        [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "        [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "        [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "        [0.        , 0.        , 0.        , 0.        , 0.        ]]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# temp = calculate_cosine_similarity(embeddings_itens[6], embeddings_posts[6])\n",
        "len(embeddings_itens[6][0])\n",
        "len(embeddings_itens[6][1])\n",
        "\n",
        "len(embeddings_posts[6][100])\n",
        "\n",
        "\n",
        "len(temp[511][0])\n",
        "import numpy as np\n",
        "temp[511].numpy()"
      ],
      "metadata": {
        "id": "7BOQiTNwJCQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open('/content/drive/MyDrive/colB5BERT/embeddings_L6.pkl', 'wb') as f:\n",
        "    pickle.dump(embeddings_L6, f)"
      ],
      "metadata": {
        "id": "t-N2ZXxP9NqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Save each tuple of (topk_values, topk_indices) in the result\n",
        "for i, (topk_values, topk_indices) in enumerate(embeddings_L6):\n",
        "    np.savez(f'/content/drive/MyDrive/colB5BERT/output_{i}.npz', values=topk_values, indices=topk_indices)\n",
        "\n",
        "data = np.load('output_0.npz')\n",
        "values = data['values']\n",
        "indices = data['indices']\n"
      ],
      "metadata": {
        "id": "zM0a8wbKyGmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def save_to_csv(tensor_list, filename):\n",
        "    flattened_tensors = [tensor.numpy().flatten() for tensor in tensor_list]\n",
        "    shapes = [tensor.shape for tensor in tensor_list]\n",
        "\n",
        "    data_df = pd.DataFrame({\n",
        "        'tensor': [tensor.tolist() for tensor in flattened_tensors],\n",
        "        'shape': shapes\n",
        "    })\n",
        "\n",
        "    data_df.to_csv(filename, index=False)\n",
        "\n",
        "# Assuming your list of tensors is called tensor_list\n",
        "save_to_csv(embeddings_itens[6], 'embeddings_itens6.csv')\n",
        "\n"
      ],
      "metadata": {
        "id": "C9Ut022SSFng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the pickle file\n",
        "with open(\"/content/drive/MyDrive/colB5BERT/embeddings_L6.pkl\", \"rb\") as file:\n",
        "    data = pickle.load(file)\n",
        "\n",
        "type(data)\n",
        "len(data)\n",
        "len(data[1][0])\n",
        "\n",
        "\n",
        "cosims_L6 = [y.numpy() for y in [x[0] for x in data]]\n",
        "\n",
        "# Save the result as a new pickle file\n",
        "with open(\"/content/drive/MyDrive/colB5BERT/cosim_L6.pkl\", \"wb\") as file:\n",
        "    pickle.dump(cosims_L6, file)\n",
        "\n",
        "ids_cosims_L6 = [y.numpy() for y in [x[1] for x in data]]\n",
        "\n",
        "ids_cosims_L6[0:2]\n",
        "\n",
        "len(cosims_L6)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/colB5BERT/ids_cosims_L6.pkl\", \"wb\") as file:\n",
        "    pickle.dump(ids_cosims_L6, file)\n",
        "\n",
        "\n",
        "with open(\"/content/drive/MyDrive/colB5BERT/cosim_L6.pkl\", \"wb\") as file:\n",
        "    pickle.dump(cosims_L6, file)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/colB5BERT/cosim_L6_full.pkl\", \"wb\") as file:\n",
        "    pickle.dump(embeddings_L6, file)\n"
      ],
      "metadata": {
        "id": "mdrdFvvoKcPd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "qwNE2XOqGhuV"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}