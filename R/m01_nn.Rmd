---
title: "Model 1: Fully connected neural network with document term/feature matrix with tf_idf"
output: html_document
---


```{r}
library(tidyverse)
library(tidytext)

library(psych)
library(keras)
library(tensorflow)
library(tm)
library(SnowballC)
library(quanteda)
library(reticulate)
# devtools::install_github("kbenoit/quanteda")


```


```{r}

install.packages("keras")
tensorflow::install_tensorflow()
keras::install_keras()
conda_list()
py_available()

```



#### Prepare data
```{r}


db_splited_embeddings <- db_splited_embeddings %>%
  mutate(id = as.numeric(id)) %>%
  left_join({
    db_bfi %>% select(id, O_rec:N_vlti_rec)
  }, by = "id")

glimpse(db_splited_embeddings)
names(db_splited_embeddings)

db_splited_embeddings[, 4:108] %>% skimr::skim()


#PQP
map(db_splited_embeddings[, 4:108],  ~ table(is.na(.x)))
map(db_splited_embeddings[, 4:108],  ~ table(is.nan(.x)))
db_splited_embeddings <-
  db_splited_embeddings %>% filter(!is.na(`1`))

# db_splited_embeddings <-  na.omit(db_splited_embeddings)



f_aux <- function(x) {
  x <- as.numeric(scale(x))
  x <- ifelse(x < -4,-4, x)
  x <- ifelse(x > 4, 4, x)
  return(x)
}


db_splited_embeddings[, 4:108] <-
  apply(db_splited_embeddings[, 4:108], MARGIN = 2, FUN =  f_aux)



# Shuffle data
set.seed(4)

n <- dim(db_splited_embeddings)[1]
indices <- sample(1:n)

prop_train <- .80
train_indices <- 1:round(prop_train * n, 0)
val_indices <-  (round(prop_train * n, 0) + 1):n


x_train <-
  db_splited_embeddings[indices[train_indices], 4:103] %>% as.matrix
y_train <-
  db_splited_embeddings[indices[train_indices], 104:108] %>% as.matrix

x_val <-
  db_splited_embeddings[indices[val_indices], 4:103] %>% as.matrix
y_val <-
  db_splited_embeddings[indices[val_indices], 104:108] %>% as.matrix

dim(x_train)
dim(x_val)


```

### Data from lexical features

```{r}


db_splited_embeddings <- db_splited_embeddings %>%
  mutate(id = as.numeric(id)) %>%
  left_join(lexical, by = "id")

glimpse(db_splited_embeddings)
names(db_splited_embeddings)

db_splited_embeddings[, 122:280] %>% skimr::skim()


f_aux <- function(x) {
  x <- as.numeric(scale(x))
  x <- ifelse(x < -4,-4, x)
  x <- ifelse(x > 4, 4, x)
  return(x)
}


db_splited_embeddings[, 122:280] <-
  apply(db_splited_embeddings[, 122:280], MARGIN = 2, FUN =  f_aux)

db_splited_embeddings <- na.omit(db_splited_embeddings)

map(db_splited_embeddings[, 122:280],  ~ table(is.na(.x)))
map(db_splited_embeddings[, 122:280],  ~ table(is.nan(.x)))

map(db_splited_embeddings[, 104:108],  ~ table(is.nan(.x)))
map(db_splited_embeddings[, 104:108],  ~ table(is.na(.x)))

names(db_splited_embeddings)

db_splited_embeddings <- db_splited_embeddings %>%
  ungroup() %>%
  dplyr::filter(if_any(c(104:108, 122:280), ~ !is.na(.)))

db_splited_embeddings$`70`

# Shuffle data
set.seed(4)

n <- dim(db_splited_embeddings)[1]
indices <- sample(1:n)

prop_train <- .80
train_indices <- 1:round(prop_train * n, 0)
val_indices <-  (round(prop_train * n, 0) + 1):n


x_train <-
  db_splited_embeddings[indices[train_indices], 122:280] %>% as.matrix
y_train <-
  db_splited_embeddings[indices[train_indices], 104:108] %>% as.matrix

# y_train <- to_one_hot(y_train)

x_val <-
  db_splited_embeddings[indices[val_indices], 122:280] %>% as.matrix
y_val <-
  db_splited_embeddings[indices[val_indices], 104:108] %>% as.matrix


dim(x_train)
dim(x_val)

is.nan(x_train) %>% table
is.na(x_val) %>% table

is.na(y_train) %>% table
is.na(y_val) %>% table


replace_na_random_values <- function(mat) {
  # Assuming mat is your matrix
  min_value <- min(mat, na.rm = T)  # Set your minimum value
  max_value <- max(mat,  na.rm = T)  # Set your maximum value
  
  # Identify NAs in the matrix
  na_elements <- is.na(mat)
  
  # Replace NAs with random numbers
  mat[na_elements] <-
    runif(sum(na_elements), min = min_value, max = max_value)
  
  return(mat)
  
}


na_indices <- which(is.na(x_train), arr.ind = TRUE)

x_train <-  replace_na_random_values(x_train)

    
```


#### Model arquictecture definition 
```{r}

model <- keras_model_sequential() %>%
  layer_dense(
    units = 100,
    activation = "relu",
    input_shape = c(159),
    kernel_regularizer = regularizer_l2(0.001)
  ) %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(
    units = 100,
    activation = "relu",
    input_shape = c(100),
    kernel_regularizer = regularizer_l2(0.001)
  ) %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 5)

summary(model)

model <- keras_model_sequential() %>%
  layer_dense(
    units = 5,
    input_shape = c(159),
    kernel_regularizer = regularizer_l2(0.001)
  )



```

#### Optimizer and metrics of accurary

```{r}

model %>% compile(
  optimizer = optimizer_rmsprop(learning_rate = 0.001),
  loss = 'mean_squared_error',
  metrics = c("mae")
)


```
#### Learning rate scheduler
```{r}
lr_schedule <- function(epoch, lr) {
  if (epoch >= 5) {
    lr * 0.5
  } else {
    lr
  }
}

callback_learning_rate_scheduler <- callback_learning_rate_scheduler(
  schedule = lr_schedule
)


```

#### Training the model
```{r}


history <- model %>% fit(
  x_train,
  y_train,
  epochs = 50,
  batch_size = 128,
  validation_data = list(x_val, y_val),
  callbacks = list(callback_learning_rate_scheduler)
)

plot(history)
```

#### Ridge regression

```{r}


df <- db_bfi %>%
  mutate(id = as.numeric(id)) %>%
  left_join(lexical , by = "id") %>%
  dplyr::filter(if_all(c(65:69, 108:266), ~ !is.na(.)))

set.seed(123)

corr.test(df[, 108:266], df[, 65:69])

names(df)
train_indices <- df %>%
  sample_frac(.8) %>%
  pull(id)


x_train <- df %>%
  filter(id %in% train_indices) %>%
  select(allTypes:V3p) %>%
  as.matrix

x_val <- df %>%
  filter(!(id %in% train_indices)) %>%
  select(allTypes:V3p) %>%
  as.matrix

y_train <- df %>%
  filter(id %in% train_indices) %>%
  select(O_rec:N_rec) %>%
  as.matrix

y_val <- df %>%
  filter(!(id %in% train_indices)) %>%
  select(O_rec:N_rec) %>%
  as.matrix


model <- keras_model_sequential() %>%
  layer_dense(
    units = 5,
    activation = "linear",
    input_shape = dim(x_train)[2],
    kernel_regularizer = regularizer_l2(0.1)
  )


model <- keras_model_sequential() %>%
  layer_dense(
    units = 5,
    activation = "linear",
    input_shape = dim(x_train)[2],
    kernel_regularizer = regularizer_l2(0.1)
  )

model %>% compile(optimizer = optimizer_rmsprop(),
                  loss = "mse")

model %>% fit(
  x_train,
  y_train,
  input_shape = c(156),
  epochs = 50,
  validation_split = 0.2
)


```


#### Exploring the results of nn

```{r}

    predictions <-  model %>% predict(x_val)
    vars <-   dimnames(y_val) [[2]] 
   
    dimnames(predictions)[[2]] <- paste0(vars, "_pred")
   
   
    # results <-  model %>% evaluate(x_val, y_val)
   
    df_test <- db_splited_embeddings[indices[val_indices], ] %>% 
      bind_cols( predictions) %>% ungroup()
    
  
   
     glimpse(df_test)
     df_test %>% select(all_of(c("id", vars, paste0(vars, "_pred")))) %>% 
       group_by(id) %>%
       summarise(across(O_rec:N_rec_pred, .fns=mean)) %>%
       corr.test() 
    
  
 
    dev.new()
    psych::pairs.panels(df)
    dev.copy(png, "yourfilename.png")
    
  
    dev.new()
    ggplot( df, aes(x =  E_rec_pred , y = E_rec ) ) +
        geom_point( alpha = 1/2) +
        geom_smooth(method = "lm") +
        geom_smooth(color = "red")
    dev.copy(png, "yourfilename.png")
 

```
#### Exploring the results of ridge regression
```{r}
 predictions <-  model %>% predict(x_val)
 predictions <-  model %>% predict(x_train)
 
 vars <-   dimnames(y_val) [[2]] 
 dimnames(predictions)[[2]] <- paste0(vars, "_pred")
   
  df %>% filter(!(id %in% train_indices)) %>% 
      bind_cols( predictions) %>% 
      select(all_of(c(vars, paste0(vars, "_pred")))) %>%
       corr.test() 
  
   df %>% filter(id %in% train_indices) %>% 
      bind_cols( predictions) %>% 
      select(all_of(c(vars, paste0(vars, "_pred")))) %>%
       corr.test() 
    
```



##### Conclusions



